<h1 id="httpssiversorgbookfluentforever"><a href="https://sivers.org/book/FluentForever">https://sivers.org/book/FluentForever</a></h1>

<p>first 625: <a href="http://fluent-forever.com/wp-content/uploads/2014/05/625-List-Thematic.pdf">http://fluent-forever.com/wp-content/uploads/2014/05/625-List-Thematic.pdf</a></p>

<ul>
<li>start off with “minimal pair testing” (e.g. having to differentiate sounds like niece and knees)</li>
<li>use Google images, not translations</li>
<li>to memorize genders(/tones/…) imagine the masculine terms exploding, feminine catching fire, neuter shattering like glass (similar for tone colors)</li>
<li>good at remembering when images are violent/sexual/funny</li>
<li>cloze card types for functional words like “of”, “what”, …</li>
<li>for 10 ways to form plural, pick 10 nouns and use the person-action-object system (Tiger essen Fleisch)</li>
<li>also use this to learn verb/noun/adjective/adverb patterns</li>
<li>submit sentences to lang8 and put corrections in flash cards</li>
<li>TV series are easier than films (read ahead on Wikipedia, no subs)</li>
<li>card types should also ask for mnemonic (e.g. “What’s a phrase that includes [word]?”)</li>
</ul>



<h1 id="httparxivorgpdf151106444pdf"><a href="http://arxiv.org/pdf/1511.06444.pdf">http://arxiv.org/pdf/1511.06444.pdf</a></h1>

<p>universality in halting time for spin glasses and deep neural networks</p>



<h1 id="httparxivorgpdf13013537pdf"><a href="http://arxiv.org/pdf/1301.3537.pdf">http://arxiv.org/pdf/1301.3537.pdf</a></h1>

<p>super short abstract:</p>

<ul>
<li>pooling operators are for local invariance</li>
<li>1 layer of convolutions is for learning 1 one-parameter-group invariance (or rather: convolutions leave already learned group invariance untouched and the training learns the invariance)</li>
<li>deep networks are basically doing group factorisation that’s stable w.r.t. perturbations of the group</li>
</ul>

<p>my notes:</p>

<ul>
<li>the problem: find signal representation <script type="math/tex" id="MathJax-Element-167">\Phi</script> (e.g. classifier) that is <br>
<ul><li>invariant under some transformation group <script type="math/tex" id="MathJax-Element-168">G</script> (e.g. rotation), i.e. <script type="math/tex" id="MathJax-Element-169">\Phi(x)=\Phi(gx)</script> for all <script type="math/tex" id="MathJax-Element-170">g\in G</script> and</li>
<li>is “stable” to perturbations (i.e. transformations <script type="math/tex" id="MathJax-Element-171">h</script> that are “close” to the transformation group <script type="math/tex" id="MathJax-Element-172">G</script> which can be thought of as a low dimensional manifold), i.e. <script type="math/tex" id="MathJax-Element-173">||\Phi(\varphi(h,x))-\Phi(x)||\leq C||x||d(h,G)</script></li></ul></li>
<li>approach: take convolutional networks and think of them in terms of signal processing, i.e. <br>
<ul><li>inputs <script type="math/tex" id="MathJax-Element-174">x\in X</script> = signals <script type="math/tex" id="MathJax-Element-175">x\in L^2(\Omega)</script></li>
<li>convolution kernels = filter bank <script type="math/tex" id="MathJax-Element-176">\{\psi_\lambda\}_\lambda, \lambda\in\Lambda_1</script>, e.g. for a filter bank corresponding to an expansion in a funtions Fourier series, convolution with <script type="math/tex" id="MathJax-Element-177">\psi_\lambda</script> yields <script type="math/tex" id="MathJax-Element-178">\langle x,e^{2\pi i\lambda/N}</script></li>
<li>the first layer maps <script type="math/tex" id="MathJax-Element-179">x(\cdot)\mapsto \{x*\psi_\lambda(\cdot)\}_\lambda =: z^{(1)}(\cdot,\lambda)</script> (which is the set of <strong>convolutions</strong> with kernels <script type="math/tex" id="MathJax-Element-180">\psi_\lambda</script>, applies some operator <script type="math/tex" id="MathJax-Element-181">M</script> (the <strong>activation function</strong> which acts piece-wise) and then applies some <strong>pooling operator</strong> <script type="math/tex" id="MathJax-Element-182">P</script>, which, in terms of signal processing, can be thought of as low-pass filter (low-pass because chosing max-pooling can be thought of as eliminating high frequency oscillations), followed by downsampling (see Wikipedia for a quick explanation)</li></ul></li>
<li>the special case of one-parameter transformation groups <script type="math/tex" id="MathJax-Element-183">G=\{U_t\}_{t\in\mathbb R}, U_t\in L^2(\Omega), \lim U_t z = U_{t_0}z, U_{t+s}=U_tU_s</script> (e.g. translations, frequency transpositions, dilations): <br>
<ul><li>we want to find a canonical way of describing the action of a group element of a one-parameter family; this is given by Stone’s theorem which (under reasonable assumptions) states that there is some s.a. <script type="math/tex" id="MathJax-Element-184">A</script> such that <script type="math/tex" id="MathJax-Element-185">U_t=\exp(itA)</script> (spectral theorem!)</li>
<li>since <script type="math/tex" id="MathJax-Element-186">A</script> is self adjoint there is a change of basis given by <script type="math/tex" id="MathJax-Element-187">O</script> that diagonalizes <script type="math/tex" id="MathJax-Element-188">A</script>, i.e. (in the finite dimensional case) <script type="math/tex" id="MathJax-Element-189">OAO^{-1} = \text{diag}(\lambda_1,\dots,\lambda_n)</script></li>
<li>hence we can write the group action <script type="math/tex" id="MathJax-Element-190">U_tz = O^{-1}(\exp(it\text{diag}(\lambda_1,\dots,\lambda_n))Oz)</script>, implying that the group action is a linear phase change in the basis that diagonalizes <script type="math/tex" id="MathJax-Element-191">A</script></li>
<li>choosing <script type="math/tex" id="MathJax-Element-192">M</script> to take the complex modulus yields a representation that is invariant under the action of <script type="math/tex" id="MathJax-Element-193">\{U_t\}_t</script></li></ul></li>
<li>“defining” perturbations of group elements: <br>
<ul><li>using the change of basis given by <script type="math/tex" id="MathJax-Element-194">O</script> we can write the group action as some linear phase change; if we also apply the inverse Fourier transform this becomes a translation operator <script type="math/tex" id="MathJax-Element-195">T_s:z(\cdot)\mapsto z(\cdot -s)</script>, which lets us measure deformations (<script type="math/tex" id="MathJax-Element-196">\tilde T_s: z(\cdot)\mapsto z(\cdot -\tau(s))</script> such that <script type="math/tex" id="MathJax-Element-197">d(\tilde T_s,T_s)\ll 1</script>) in a convenient way by analyzing the regularity of <script type="math/tex" id="MathJax-Element-198">\tau</script> <br>
<!--  * thus the key to obtaining group invariant representations is not to find die eigenvectors of $A$ (yielding the change of basis given by $O$), but rather measurements that are &#8220;close&#8221; to diagonalising $A$ and are localised where deformations occur(?) - this can be done with convolutions with compactly supported filters--> <br>
I do not quite understand how this motivates the use of filters(/kernels) acting locally for group factorisation, but as soon as I do this will be updated.</li></ul></li>
</ul>



<h1 id="properties-of-the-fourier-transform">Properties of the Fourier transform</h1>

<ul>
<li>diagonalises the derivative (i.e. turns it into a multiplication operator if expressed in its basis), which in turn let’s us define pseudo-differential operators via non-polynomial symbols</li>
<li>Fourier transform is like a projection onto eigenspaces of Laplace operator → in a compact domain (with suitable regularity) those are discrete → get a sum = Fourier <em>series</em> <br>
<ul><li>using the above idea one can generalize Fourier transforms not only to higher dimensional setting and manifolds, but also to graph settings (see graph Laplacian)</li></ul></li>
<li>translation = phase change (think of a shift in 1D and what this does to the coefficients of a Fourier <em>series</em>) <br>
<ul><li>in a space-time metric (having signature 1,1,1,-1) we can “place” a particle at some point in space-time by multiplying the creation operator with some exponential in the right basis (i.e. the one of the Fourier transform) where the spatial part has the inverse sign of the time part because of this metric</li></ul></li>
<li>smoothness of the original function corresponds to decay in the Fourier domain (and vice versa), which can be made precise by the following statement about a random variable’s characteristic function (which is pretty much it’s Fourier transform) <br>
<ul><li><script type="math/tex" id="MathJax-Element-199">µ({x: |x|≥2/u})≤\frac{1}{u} \int_{-u}^u (1-\hat µ(t))dt</script></li>
<li>in particular gives the following implication (in the appropriate setting): “characteristic function continuous in 0” → “measure is tight” (where tightness is useful since it enables application of Helly’s selection theorem, which is more or less the Banach-Alaoglu theorem for probability measures (latter would give weak-* convergence for signed, unnormed measures)</li></ul></li>
<li><a href="https://terrytao.wordpress.com/2009/04/06/the-fourier-transform/">https://terrytao.wordpress.com/2009/04/06/the-fourier-transform/</a> for a more general take on the subject</li>
</ul>



<h1 id="braess-paradox-httpistacatfileadminuseruploadpdfstalks201602talktimmepdf">Braess’ paradox (<a href="http://ist.ac.at/fileadmin/user_upload/pdfs/Talks/2016/02/Talk_Timme.pdf">http://ist.ac.at/fileadmin/user_upload/pdfs/Talks/2016/02/Talk_Timme.pdf</a>)</h1>

<ul>
<li>the paradox: given some (directed) graph with a flow (e.g. traffic network) removing edges could improve the “overall situation” (e.g. on average you do not drive as long as before if a street is closed)</li>
<li>note that this also implies the converse: adding a street doesn’t necessarily make the traffic situation better overall</li>
<li>one take on this paradox (from a dynamical systems point of view): knowing the capacity of all the edges and the nodes one gets a system of constraints for every closed loop in order to satisfy some stability condition, but if one introduces another edge that divides a circle in two we get two systems of constraints that may not be compatible → no more stable solution (= traffic jam)</li>
</ul>



<h1 id="httpwwwhuffingtonpostcom20150513andrew-ngn7267682html"><a href="http://www.huffingtonpost.com/2015/05/13/andrew-ng_n_7267682.html">http://www.huffingtonpost.com/2015/05/13/andrew-ng_n_7267682.html</a></h1>

<ul>
<li>“The idea is that innovation is not these random unpredictable acts of genius, but that instead one can be very systematic in creating things that have never been created before. […] learn a lot, read a lot, talk to experts.”</li>
<li>on early influences: moved around, visited many different colleges and interned at different labs → many different points of view</li>
<li>don’t “follow your passion” (which usually gets amended to “follow your passion of all the things that happen to be a major at the university you’re attending”); often “you first become good at something, and then you become passionate about it. And I think most people can become good at almost anything. So when I think about what to do with my own life, what I want to work on, I look at <em>two criteria</em>. The first is whether it’s an <em>opportunity to learn</em>. Does the work on this project allow me to learn new and interesting and useful things? The second is the <em>potential impact</em>. <strong>The world has an infinite supply of interesting problems. The world also has an infinite supply of important problems. I would love for people to focus on the latter.</strong>”</li>
<li>“one pattern of mistakes I’ve made in the past, hopefully much less now, is doing projects where you do step one, you do step two, you do step three, and then you realize that step four has been impossible all along”</li>
<li>“if you seriously study half a dozen papers a week and you do that for two years, after those two years you will have learned a lot. […] But that sort of investment, if you spend a whole Saturday studying rather than watching TV, there’s no one there to pat you on the back or tell you you did a good job. Chances are what you learned studying all Saturday won’t make you that much better at your job the following Monday. There are very few, almost no short-term rewards for these things. But it’s a fantastic long-term investment. […] People that count on willpower to do these things, it almost never works because willpower peters out. Instead I think people that are into creating habits — you know, studying every week, working hard every week — those are the most important. Those are the people most likely to succeed.”</li>
<li>“I don’t work on preventing AI from turning evil for the same reason that I don’t work on combating overpopulation on the planet Mars”, more urgent: “[…] when the U.S. transformed from an agricultural to a manufacturing and services economy, we had people move from one routine task, such as farming, to a different routine task, such as manufacturing or working call service centers. A large fraction of the population has made that transition, so they’ve been okay, they’ve found other jobs. But many of their jobs are still routine and repetitive. The challenge that faces us is to find a way to scalably teach people to do non-routine non-repetitive work. Our education system, historically, has not been good at doing that at scale. The top universities are good at doing that for a relatively modest fraction of the population. But a lot of our population ends up doing work that is important but also routine and repetitive. That’s a challenge that faces our educational system.”</li>
<li>“One thing about speech recognition: most people don’t understand the difference between 95 and 99 percent accurate. Ninety-five percent means you get one-in-20 words wrong. That’s just annoying, it’s painful to go back and correct it on your cell phone. Ninety-nine percent is game changing. If there’s 99 percent, it becomes reliable.”</li>
</ul>



<h1 id="httpwwwpaulgrahamcomhshtml"><a href="http://www.paulgraham.com/hs.html">http://www.paulgraham.com/hs.html</a></h1>

<p>great text, you should read it!</p>

<ul>
<li>if you don’t have a clear goal (which is usually the case) work forward from promising situations instead</li>
<li>if you have to choose, take those options that will give you the most promising range of options afterward</li>
<li>work on hard problems (“Writing novels is hard. Reading novels isn’t. Hard means worry: if you’re not worrying that something you’re making will come out badly, or that you won’t be able to understand something you’re studying, then it isn’t hard enough.”) → also get to know interesting people and get big ideas</li>
<li>“Put in time how and on what? Just pick a project that seems interesting: to master some chunk of material, or to make something, or to answer some question. Choose a project that will take less than a month, and make it something you have the means to finish. Do something hard enough to stretch you, but only just, especially at first. If you’re deciding between two projects, choose whichever seems most fun. If one blows up in your face, start another. Repeat till, like an internal combustion engine, the process becomes self-sustaining, and each project generates the next one.”</li>
<li>“Don’t disregard unseemly motivations. One of the most powerful is the desire to be better than other people at something.”</li>
</ul>



<h1 id="httpsterrytaowordpresscom20080807on-time-management"><a href="https://terrytao.wordpress.com/2008/08/07/on-time-management/">https://terrytao.wordpress.com/2008/08/07/on-time-management/</a></h1>

<ul>
<li>“Another thing is that my ability to do any serious mathematics fluctuates greatly from day to day; sometimes I can think hard on a problem for an hour, other times I feel ready to type up the full details of a sketch that I or my coauthors already wrote, and other times I only feel qualified to respond to email and do errands, or just to take a walk or even a nap. I find it very helpful to organise my time to match this fluctuation: for instance, if I have a free afternoon, and feel inspired to do so, I might close my office door, shut off the internet, and begin typing on a languishing paper; or if not, I go and work on a week’s worth of email, referee a paper, write a blog article, or whatever else seems suited to my current levels of energy and enthusiasm. It is fortunate in mathematics that a large fraction of one’s work (with the notable exception of teaching, which one then has to build one’s schedule around) can be flexibly moved from one time slot to another in this manner. [A corollary to this is that one should deal with tasks before they become so urgent that they have to be done immediately, thus disrupting one’s time flexibility.]”</li>
<li>“A half-hearted system is probably worse than no system at all. A corollary to this is not to try to make an overly ambitious system ab nihilo that one is unlikely to follow faithfully; it is probably better to let such systems evolve over time.“</li>
<li>“Sometimes one should abandon one’s own rules and allow for serendipity. There have been many times, for instance, when I had planned to work on something during my lunch hour (grabbing something quick to eat), when I was interrupted by a colleague or visitor to go out to eat. It has often happened that I got a lot more out of that lunch (mathematically or otherwise) than I would have back at the office, though not in the way I would have anticipated.”</li>
</ul>






=====================YET TO WRITE=======================
========================================================









# http://arxiv.org/pdf/1405.4537v1.pdf
* def: stream is a map <script type="math/tex" id="MathJax-Element-200">\gamma</script> from a totally ordered set <script type="math/tex" id="MathJax-Element-201">I</script> to some state space
* there is a canonical way to convert discrete streams to continuous paths (path: <script type="math/tex" id="MathJax-Element-202">I</script> interval and some regularity conditions like right continuity)
* from now on “wlog”: <script type="math/tex" id="MathJax-Element-203">\gamma:[J_-,J_+]\rightarrow E</script> will continuously map an interval <script type="math/tex" id="MathJax-Element-204">J</script> to some Banach space <script type="math/tex" id="MathJax-Element-205">E</script>









* def: bounded <script type="math/tex" id="MathJax-Element-206">p</script>-variation (<script type="math/tex" id="MathJax-Element-207">p\geq 1</script>) iff <script type="math/tex" id="MathJax-Element-208">sup_{\dots<u_i < u_{i+1}<\dots\in J}\sum_i\|\gamma_{u_{i+1}}-\gamma_{u_i}\|^q <\infty</script> for <script type="math/tex" id="MathJax-Element-209">q=1</script> and <script type="math/tex" id="MathJax-Element-210">q=p</script>
========================================================









===================TO BE CONTINUED======================
========================================================
--&gt;



<h1 id="httpmathucreduhomebaezrosettapdf"><a href="http://math.ucr.edu/home/baez/rosetta.pdf">http://math.ucr.edu/home/baez/rosetta.pdf</a></h1>

<p>great paper for everybody who is interested in (yet not familiar with) category theory - summary may follow</p>



<h1 id="uses-of-的-中文">uses of 的 (中文)</h1>

<ul>
<li>possessive particle: 我的女 = my daughter</li>
<li>attributive (connecting adjective and noun): 红色的菜 = red vegetables </li>
</ul>



<h1 id="httparxivorgpdfcond-mat0611023v1pdf"><a href="http://arxiv.org/pdf/cond-mat/0611023v1.pdf">http://arxiv.org/pdf/cond-mat/0611023v1.pdf</a></h1>

<ul>
<li>distribution of eigenvalues of the Hessian of a critical point is a shifted semicircle</li>
<li>e.g. global minimum → left of SCL is at 0, the bigger its energy, the more it is shifted to the left)</li>
</ul>



<h1 id="asking-yesno-questions-中文">asking yes/no questions (中文)</h1>

<ul>
<li>there are two ways of doing so, either with <br>
<ul><li>吗: e.g. 吃飽了嗎？ (have you eaten?), or</li>
<li>X不X: 你要不要去北京？ (Do you or don’t you want to go to Beijing?) which is equivalent to 你要去北京嗎？</li></ul></li>
</ul>



<h1 id="past-tense-了-vs-过-中文">past tense: 了 vs 过 (中文)</h1>

<ul>
<li>过: applicable if action is repeatable and is finished</li>
<li>了: applicable for events which started in the past and continue to the present</li>
<li>e.g. 她去过美国｡vs 她去美国了｡ (过→she is not there anymore, 了→she is still there)</li>
</ul>



<h1 id="expressing-priorposterior-actions-with-以前以后-中文">expressing prior/posterior actions with 以前/以后 (中文)</h1>

<ul>
<li>as “previously”/“after”: <br>
<ul><li>他以前住在美国｡ (He (previously) lived in America.)</li>
<li>他以后会去美国｡ or 以后他会去美国｡ (He will go to America (afterwards).) — note that 以后 can be before or after the subject!</li></ul></li>
<li>relatively prior to/after something else: <br>
<ul><li>[action]以前[prior action], e.g. 他睡觉以前喜欢看书｡(He likes to read before going to bed. (literally – He sleep before, likes read.))</li>
<li>[action]以后[later action], e.g. 他下课以后要回家吃饭｡(After class, he will return home to eat. (literally – He class is over after, going to return home eat a meal.))</li></ul></li>
</ul>



<h1 id="httpmathstackexchangecomquestions766479what-is-spectrum-for-laplacian-in-mathbbrn-httpmathstackexchangecomquestions790401spectrum-of-laplace-operator"><a href="http://math.stackexchange.com/questions/766479/what-is-spectrum-for-laplacian-in-mathbbrn">http://math.stackexchange.com/questions/766479/what-is-spectrum-for-laplacian-in-mathbbrn</a>, <a href="http://math.stackexchange.com/questions/790401/spectrum-of-laplace-operator">http://math.stackexchange.com/questions/790401/spectrum-of-laplace-operator</a></h1>

<p>two great articles covering why the spectrum of the Laplace operator (on <script type="math/tex" id="MathJax-Element-345">\mathbb R^N</script>) is <script type="math/tex" id="MathJax-Element-346">(-∞,0]</script> which, by looking more closely, also explains why “unbounded domain” implies “uncountable basis of eigenfunctions”</p>



<h1 id="httpcs231ngithubioconvolutional-networks"><a href="http://cs231n.github.io/convolutional-networks/">http://cs231n.github.io/convolutional-networks/</a></h1>

<p>great introduction to convolutional neural networks</p>



<h1 id="httpstackoverflowcomquestions7536465create-a-2d-array-with-a-nested-loop"><a href="http://stackoverflow.com/questions/7536465/create-a-2d-array-with-a-nested-loop">http://stackoverflow.com/questions/7536465/create-a-2d-array-with-a-nested-loop</a></h1>

<p>why you want to use <code>[[None for j in xrange(3)] for i in xrange(3)]</code> instead of <code>[[None]*3]*3</code></p>



<h1 id="dft">DFT</h1>

<ul>
<li>DFT in <script type="math/tex" id="MathJax-Element-347">N</script> dimensions can be written as <script type="math/tex" id="MathJax-Element-348">F_N=1/\sqrt N (\omega_N^{kl})_{k,l=0}^{N-1}</script>, where <script type="math/tex" id="MathJax-Element-349">\omega_N=e^{2\pi i/N}</script> and this matrix is a Vandermonde matrix over the roots of unity</li>
<li>multidimensional DFT consists of iterated sums which commute → can write it as <script type="math/tex" id="MathJax-Element-350">F_{N_2}(F_{N_1}(X_{i,j})_{j=0}^{N_1-1})_{j=0}^{N_2-1}</script></li>
<li>Vandermonde matrix = evaluation of a polynomial at points generating the Vandermonde matrix → upsampling = (e^2πinm/2N)_{n,m=0}^{n=N-1,m=2N-1} (Vandermonde matrix with twice as many rows as a square one → evaluates the polynomial described by the given vector at twice as many points of the unit circle (→ to convert a signal of frequency a to one with frequency b you first need to upsample it to LCM(a,b))</li>
<li>real signal x → DFT(x) has entries that are complex conjugated to each other (usually mirrored around half the length of the signal) since they are just the <script type="math/tex" id="MathJax-Element-351">L^2(\mathbb C)</script> inner product <script type="math/tex" id="MathJax-Element-352">(\langle x,e_k\rangle)_k</script></li>
</ul>



<h1 id="linearisation-of-a-differential-operator">linearisation of a differential operator</h1>

<p><a href="http://math.stackexchange.com/questions/1677181/how-do-you-linearize-a-differential-operator-to-get-its-symbol">http://math.stackexchange.com/questions/1677181/how-do-you-linearize-a-differential-operator-to-get-its-symbol</a> for an example</p>

<ul>
<li>used (i.a.) to define the symbol of a differential operator <script type="math/tex" id="MathJax-Element-494">D</script></li>
<li>how to do it: <br>
<ul><li>usually linearize around a solution <script type="math/tex" id="MathJax-Element-495">\tilde u, D\tilde u=0</script></li>
<li>idea is to look at how <script type="math/tex" id="MathJax-Element-496">D</script> behaves on functions <script type="math/tex" id="MathJax-Element-497">u</script> that are close to the point one is linearising around (i.e. <script type="math/tex" id="MathJax-Element-498">\tilde u</script>), so one does the substitution <script type="math/tex" id="MathJax-Element-499">u=\tilde u+\varepsilon v</script></li>
<li>rearranging terms like a power series in <script type="math/tex" id="MathJax-Element-500">\varepsilon</script> and using that <script type="math/tex" id="MathJax-Element-501">D\tilde u=0</script> one gets equations with <script type="math/tex" id="MathJax-Element-502">\tilde u</script> and <script type="math/tex" id="MathJax-Element-503">v</script> which define an operator <script type="math/tex" id="MathJax-Element-504">A_{\tilde u}</script> acting on <script type="math/tex" id="MathJax-Element-505">v</script>, which satisfies <script type="math/tex" id="MathJax-Element-506">A_{\tilde u}v = 0</script> and is linear!</li></ul></li>
<li>note that the linearisation depends on the solution just as the linearisation of a scalar valued function depends on the point <script type="math/tex" id="MathJax-Element-507">x_0</script> one wants to linearize around and that the origin shifts accordingly, i.e. the input <script type="math/tex" id="MathJax-Element-508">v</script> is much like <script type="math/tex" id="MathJax-Element-509">x-x_0</script> and not <script type="math/tex" id="MathJax-Element-510">x</script></li>
</ul>



<h1 id="title"></h1>


=====================YET TO WRITE=======================
========================================================









# http://www-etud.iro.umontreal.ca/~sordonia/pdf/sigir2013_sordoni.pdf
* problem: model dependencies of words
  * either: need additional features (“computer”, “architecture” and “computer architecture” are completely different entries)









  * or: model it as joint probabilities (less improvements than expected, huge computational effort)
======================================================== 









=======================WRITE IT=========================
========================================================
--&gt;



<h1 id="httparxivorgpdf14111792v1pdf"><a href="http://arxiv.org/pdf/1411.1792v1.pdf">http://arxiv.org/pdf/1411.1792v1.pdf</a></h1>

<p>first few layers of CNNs with similar purposes tend to be very similar (Gabor filters) → can copy them</p>



<h1 id="httparxivorgpdf13013583v4pdf"><a href="http://arxiv.org/pdf/1301.3583v4.pdf">http://arxiv.org/pdf/1301.3583v4.pdf</a></h1>

<p>increasing the size of neural networks only yields diminishing returns</p>



<h1 id="httparxivorgpdf13125851v5pdf"><a href="http://arxiv.org/pdf/1312.5851v5.pdf">http://arxiv.org/pdf/1312.5851v5.pdf</a></h1>

<p>how to implement the convolutions of CNNs with the FFT (feasible for large number of feature maps)</p>



<h1 id="green-white-black-and-oolong-tea">green, white, black and Oolong tea</h1>

<p><a href="http://cooking.stackexchange.com/questions/26002/what-is-the-difference-between-green-white-and-black-tea">http://cooking.stackexchange.com/questions/26002/what-is-the-difference-between-green-white-and-black-tea</a> for quick introduction (also Oolong tea)</p>

<ul>
<li>fermentation: green&amp;white tea leaves are quickly heated (typically steamed in Japan, while pan-fired in China) after harvesting to reduce oxidation</li>
<li>white tea <br>
<ul><li>minimally processed to leave downy hair intact</li>
<li>not oxidized → does not develop as much flavor, color, or caffeine</li></ul></li>
<li>green tea <br>
<ul><li>history</li>
<li>starting from 17th century (Europeans started importing tea) until mid 19th century green tea was about as popular as black tea (in Europe)</li>
<li>originally (2000 bc) used as medicine in China</li>
<li>in the 8th century 陸羽 (Lù Yǔ) wrote 茶經 (chájīng), the first known monograph on tea where he described tea being steamed and formed into tea bricks for storage and trade - to prepare tea one had to pulverize it before brewing → 抹茶 (matcha)</li>
<li>only later it was established</li>
<li>rolling wilted leaves breaking cell walls to speed release of aromatic substances</li></ul></li>
<li>types of tea <br>
<ul><li>China</li>
<li>珠茶 (gunpowder): rolled into little round pellets (green or Oolong tea); shiny pellets indicate freshness, little pellets are considered a mark of higher quality tea</li>
<li>龙井茶 (Lóng Jǐng tea): renowned for its high quality</li>
<li>黄山毛峰 ((Huángshān) Máo Fēng): mild-flavored, very popular tea</li>
<li>茉莉花茶 (Jasmine tea): subtly sweet and highly fragrant, stored with blossoms to acquire their scent</li>
<li>白牡丹 (Bái mǔdān): one of the most well-known white teas</li>
<li>Japan</li>
<li>煎茶 (Sencha): most popular tea in Japan (80°C, 1 min, 1.5 tablespoons (7-8 grams) per litre)</li>
<li>茎茶 (Kukicha): blend made of stems, stalks, and twigs (80°C, 40sec to 1min, 4 teaspoons per litre)</li>
<li>抹茶 (Matcha): mentioned above</li>
<li>玉露 (Gyokuro): grown under the shade rather than the full sun since the more sun, the more Catechin (bitter)</li></ul></li>
<li>aromatized tea: additional flavors, but basically loses its ability of being brewed more than once</li>
</ul>



<h1 id="showing-emphasis-with-是的-中文">showing emphasis with 是…的 (中文)</h1>

<ul>
<li>encapsulate the part to be stressed with 是…的, e.g. 我(是)來台北找朋友的｡ (I came <em>to Taipei</em> to visit a friend.)</li>
<li>in positive sentences 是 may be omitted</li>
<li>only for events in the past</li>
</ul>



<h1 id="x-xivarepsilon-iin-i-does-not-have-to-be-an-open-set"><script type="math/tex" id="MathJax-Element-370">\{x : |x|_i<\varepsilon, i\in I\}</script> does not have to be an open set</h1>

<ul>
<li>take product topology (cylinder sets <script type="math/tex" id="MathJax-Element-371">\{x : |x|_i<\varepsilon, i\in J\}</script> for <script type="math/tex" id="MathJax-Element-372">J</script> being a finite subset of <script type="math/tex" id="MathJax-Element-373">I</script> are a neighbourhood basis of <script type="math/tex" id="MathJax-Element-374">0</script>)</li>
<li>if <script type="math/tex" id="MathJax-Element-375">I</script> is not finite, no element in the neighbourhood basis is contained in the set in the heading (if the norms <script type="math/tex" id="MathJax-Element-376">|\cdot|_i</script> do not trivially depend on each other) → it is not an open set</li>
<li>examples: locally convex spaces in PDEs</li>
</ul>



<h1 id="marking-direct-objects-with-把-中文">marking direct objects with 把 (中文)</h1>

<ul>
<li>used like [subject]把[object][transitive verb]</li>
<li>e.g. 他把我的杯子打破了｡ (He broke my cup.)</li>
</ul>



<h1 id="how-to-get-compact-sets-in-non-euclidean-settings">how to get compact sets in non Euclidean settings</h1>

<ul>
<li>(one) idea: given some compact inclusion <script type="math/tex" id="MathJax-Element-377">i</script> of space <script type="math/tex" id="MathJax-Element-378">X</script> into <script type="math/tex" id="MathJax-Element-379">Y</script>, find some bounded set <script type="math/tex" id="MathJax-Element-380">B\subset X</script> and consider <script type="math/tex" id="MathJax-Element-381">i(B)</script></li>
<li>this is useful for locally convex spaces of all sorts and works well with machinery like the Rellich–Kondrachov theorem</li>
</ul>



<h1 id="exp-of-involutions">exp of involutions</h1>

<ul>
<li>for any involution <script type="math/tex" id="MathJax-Element-382">a:a^2=I</script>, where <script type="math/tex" id="MathJax-Element-383">I</script> is the identity, one has <script type="math/tex" id="MathJax-Element-384">exp(ca)=cosh(c)+a sinh(c)</script> for scalar <script type="math/tex" id="MathJax-Element-385">c</script>, which can be seen by looking at the power series</li>
<li>can also be used for sin/cos with exp=i*sin+cos</li>
<li>can also be written as <script type="math/tex" id="MathJax-Element-386">cosh(c)(1+a tanh(c))</script> which can be seen as “low temperature expansion” (think about <script type="math/tex" id="MathJax-Element-387">c</script> as some temperature close to zero or more mathematically as some small <script type="math/tex" id="MathJax-Element-388">\varepsilon>0</script>)</li>
</ul>



<h1 id="geometric-interpretation-of-the-conditional-expectation">geometric interpretation of the (conditional) expectation</h1>

<ul>
<li>introduce conditional expectation <script type="math/tex" id="MathJax-Element-829">\mathbb E[X|\mathcal G]</script> as follows: <br>
<ul><li>let <script type="math/tex" id="MathJax-Element-830">X</script> be an integrable random variable on some probability space <script type="math/tex" id="MathJax-Element-831">(\Omega,\mathcal A, P)</script> </li>
<li>let <script type="math/tex" id="MathJax-Element-832">\mathcal G\subset\mathcal A</script> be a sub-<script type="math/tex" id="MathJax-Element-833">\sigma</script>-algebra</li>
<li>conditional expectation of <script type="math/tex" id="MathJax-Element-834">X</script> given <script type="math/tex" id="MathJax-Element-835">\mathcal G</script>, <script type="math/tex" id="MathJax-Element-836">\mathbb E[X|\mathcal G]</script> is <em>any</em> random variable <script type="math/tex" id="MathJax-Element-837">Y</script> s.t.</li>
<li><script type="math/tex" id="MathJax-Element-838">Y</script> is <script type="math/tex" id="MathJax-Element-839">\mathcal G</script>-measurable</li>
<li><script type="math/tex" id="MathJax-Element-840">\forall A\in\mathcal G: \mathbb E[X\bf 1_A]=\mathbb E[Y\bf 1_A]</script></li></ul></li>
<li>it can be seen that such a <script type="math/tex" id="MathJax-Element-841">Y</script> exists and, up to <script type="math/tex" id="MathJax-Element-842">P</script>-null equivalence, is uniquely determined by those conditions</li>
<li>moreover we have all the “typical” properties of expectations like <br>
<ul><li>positivity: <script type="math/tex" id="MathJax-Element-843">X\geq 0</script> implies <script type="math/tex" id="MathJax-Element-844">\mathbb E[X|\mathcal G]\geq 0</script> <script type="math/tex" id="MathJax-Element-845">P</script>-a.s.</li>
<li>linearity</li>
<li>monotonicity</li>
<li>Jensen’s inequality: <script type="math/tex" id="MathJax-Element-846">\phi(\mathbb E[X|\mathcal G])\leq\mathbb E[\phi(X)|\mathcal G]</script>, <script type="math/tex" id="MathJax-Element-847">P</script>-a.s. for <script type="math/tex" id="MathJax-Element-848">\phi</script> convex and <script type="math/tex" id="MathJax-Element-849">X</script> s.t. <script type="math/tex" id="MathJax-Element-850">\mathbb E|X|<\infty</script>, <script type="math/tex" id="MathJax-Element-851">\mathbb E|\phi(X)|<\infty</script></li>
<li>importantly: <script type="math/tex" id="MathJax-Element-852">X</script> being <script type="math/tex" id="MathJax-Element-853">\mathcal G</script>-measurable, (<script type="math/tex" id="MathJax-Element-854">\mathbb E|X|,\mathbb E|Y|<\infty</script>) implies <script type="math/tex" id="MathJax-Element-855">\mathbb E[XY|\mathcal G]=X\mathbb E[Y|\mathcal G]</script>, <script type="math/tex" id="MathJax-Element-856">P</script>-a.s., which, after identifying the deterministic “random’’ variable <script type="math/tex" id="MathJax-Element-857">X(\omega)=1 \forall\omega</script> with <script type="math/tex" id="MathJax-Element-858">1\in\mathbb R</script>, gives the second axiom for an expectation in terms of free probability, i.e. <script type="math/tex" id="MathJax-Element-859">\mathbb E[1|\mathcal G]=1</script> (the first one being linearity)</li></ul></li>
<li>one interpretation of this conditional expectation is a geometric one which can be made precise under suitable technical assumptions (<script type="math/tex" id="MathJax-Element-860">X\in L^2(\Omega,\mathcal A,P)</script>): <script type="math/tex" id="MathJax-Element-861">\mathbb E[X\mathcal G]</script> is the orthogonal projection of <script type="math/tex" id="MathJax-Element-862">X</script> from <script type="math/tex" id="MathJax-Element-863">L^2(\Omega,\mathcal A,P)</script> on <script type="math/tex" id="MathJax-Element-864">L^2(\Omega,\mathcal G,P)</script></li>
<li>to put this in words: start with some random variable (/function of degrees of freedom determined by <script type="math/tex" id="MathJax-Element-865">\mathcal A</script>) <script type="math/tex" id="MathJax-Element-866">X</script>, then find the function <script type="math/tex" id="MathJax-Element-867">\tilde X</script> in the space of functions with fewer degrees of freedom (i.e. those given by <script type="math/tex" id="MathJax-Element-868">\mathcal G</script> which has to be a subset of <script type="math/tex" id="MathJax-Element-869">\mathcal A</script>) that is “closest’’ to the original one</li>
<li>to illustrate what is meant by “degrees of freedom’’ it is instructive to look at the example <script type="math/tex" id="MathJax-Element-870">\mathcal G=\{\emptyset,\Omega\}</script>; to be <script type="math/tex" id="MathJax-Element-871">\mathcal G</script> measurable now means that the pre-image of every open set in the domain of <script type="math/tex" id="MathJax-Element-872">X</script> has to be an element of <script type="math/tex" id="MathJax-Element-873">\mathcal G</script>, which, for this <script type="math/tex" id="MathJax-Element-874">\mathcal G</script>, implies that there are only constant functions - the second property in the definition now gives that this constant value has to be the classical expectation</li>
<li>conversely, if we take a bigger <script type="math/tex" id="MathJax-Element-875">\sigma</script>-algebra, we give ourselves more degrees of freedom for <script type="math/tex" id="MathJax-Element-876">\tilde X</script> and can, in a sense stay closer to the original <script type="math/tex" id="MathJax-Element-877">X</script>, which, from a probabilistic point of view, just means that more randomness is retained - so if you think of some filtration, encoding e.g. the “knowledge’’ after flipping a coin for the <script type="math/tex" id="MathJax-Element-878">n</script>-th time, and the random variables <script type="math/tex" id="MathJax-Element-879">S_n=X_1+\dots+X_n</script> being the sum of all outcomes up to the <script type="math/tex" id="MathJax-Element-880">n</script>-th flip, conditioning on the first element of the filtration (i.e. the <script type="math/tex" id="MathJax-Element-881">\sigma</script>-algebra <script type="math/tex" id="MathJax-Element-882">\mathcal G_1=\sigma(\{(0,\{0,1\},\{0,1\},\dots),(1,\{0,1\},\{0,1\},\dots)\})</script>) just means that <script type="math/tex" id="MathJax-Element-883">X_1</script> is measurable, so <script type="math/tex" id="MathJax-Element-884">\mathbb E[X_1+X_2+\dots+X_n|\mathcal G_1]=\mathbb E[X_1|\mathcal G_1]+\mathbb E[X_2+\dots+X_n|\mathcal G_1]</script> by linearity and <script type="math/tex" id="MathJax-Element-885">E[X_1|\mathcal G_1]</script> simplifies to <script type="math/tex" id="MathJax-Element-886">X_1</script>, whereas <script type="math/tex" id="MathJax-Element-887">\mathbb E[X_2+\dots+X_n|\mathcal G_1]</script> simply becomes the usual expectation <script type="math/tex" id="MathJax-Element-888">1/2</script></li>
</ul>

<h1 id="when-are-chernoff-bounds-sharp">when are Chernoff bounds sharp?</h1>

<ul>
<li>Chernoff-type bounds are something of the form <script type="math/tex" id="MathJax-Element-435">P(X\geq a)\leq\mathbb E[e^{tX}]/e^{ta}\forall t>0</script></li>
<li>this can easily be seen by writing <script type="math/tex" id="MathJax-Element-436">P(X\geq a)\leq P(e^{tX}\geq e^{ta})</script> and applying Markov’s inequality</li>
<li>a geometric interpretation: <br>
<ul><li>write the probability of some event <script type="math/tex" id="MathJax-Element-437">A</script> as expectation of its indicator function <script type="math/tex" id="MathJax-Element-438">\bf 1_A</script></li>
<li>bounding the probability of <script type="math/tex" id="MathJax-Element-439">X\geq a</script> with <script type="math/tex" id="MathJax-Element-440">e^{tX}\geq e^{ta}</script> should be thought of as some exponential function touching the outer left part of the indicator function - which is, a priori, obviously not a bound that one expects to be sharp</li></ul></li>
<li>in the proof of Cramer’s theorem (LDP), however, we only look at random variables that are “exponentially’’ concentrated around their mean (that’s why we need large deviations after all), so the intuition is that the Chernoff bound is sharp since the whole mass is concentrated just around the point where the exponential function touches the indicator function</li>
</ul>



<h1 id="existence-of-ldps-by-subadditivity">existence of LDPs by subadditivity</h1>

<ul>
<li>assume <script type="math/tex" id="MathJax-Element-441">π_n</script> is some sequence of events we want to get a LDP for, e.g. <script type="math/tex" id="MathJax-Element-442">π_n = P(S_n≥nu)</script> for some fixed <script type="math/tex" id="MathJax-Element-443">u</script> and <script type="math/tex" id="MathJax-Element-444">S_n</script> being some sum of random variables</li>
<li>if we can show that <script type="math/tex" id="MathJax-Element-445">π_{n+m}≥π_n π_m</script> (subadditivity - check this e.g. for sum of independent r.v.), we can transform them to <script type="math/tex" id="MathJax-Element-446">p_n := -ln π_n</script> satisfying <script type="math/tex" id="MathJax-Element-447">p_{n+m}≤p_n p_m</script> and use Fekete’s subadditive lemma (<script type="math/tex" id="MathJax-Element-448">a_n≥0</script> subadditive → <script type="math/tex" id="MathJax-Element-449">\lim_{n\rightarrow\infty}a_n/n = \inf_{n\geq 1} a_n/n</script> to show that the LDP exists</li>
</ul>



<h1 id="basic-results-from-extreme-value-theory">basic results from extreme value theory</h1>

<ul>
<li><script type="math/tex" id="MathJax-Element-450">\sup_{n\leq k} X_n ~ \sigma\sqrt{2\ln k}</script>, <script type="math/tex" id="MathJax-Element-451">X_n</script> being Gaussian with variance \sigma</li>
<li>fluctuations are of inverse order <script type="math/tex" id="MathJax-Element-452">(\sigma\sqrt{2\ln k})^{-1}</script></li>
</ul>



<h1 id="httpwwwcscolumbiaeducgpdfs1180993110-laplacianpdf"><a href="http://www.cs.columbia.edu/cg/pdfs/1180993110-laplacian.pdf">http://www.cs.columbia.edu/cg/pdfs/1180993110-laplacian.pdf</a></h1>

<ul>
<li>paper discussing different variants of discrete Laplacians and their properties like symmetry, positivity, positive definiteness, locality and maximum principles</li>
<li>main result: there are graphs that do not admit all those properties simultaneously (argues with dual graphs)</li>
</ul>



<h1 id="eigenvalues-of-the-discrete-laplacian-or-generally-translation-invariant-and-symmetric-operators">eigenvalues of the discrete Laplacian (or generally translation invariant and symmetric operators)</h1>

<ul>
<li>1D case: writing out the Laplacian in the standard basis as a matrix it is easy to see that it is a circulant matrix and thus diagonalised by the discrete Fourier transform</li>
<li>general case (take 1): see <a href="http://mathoverflow.net/a/2829/47059">http://mathoverflow.net/a/2829/47059</a>, which basically says that the eigenvectors(/functions) of the Fourier transform (exp) also happen to be eigenvectors(/functions) of the translation operator → diagonalize with DFT to get eigenvalues</li>
<li>(slightly less) general case (take 2): write the action of the (discrete) Laplacian on some vector as a convolution → we know that the Fourier transformation turns convolutions into multiplications → integral operator turns into a multiplication operator, i.e. is diagonalized</li>
<li>more conrete results on <script type="math/tex" id="MathJax-Element-453">\mathbb Z</script>: from general theory about circulant matrices we know that the eigenvalues are just the Fourier transform of one row(/column since it’s symmetric) - since the Laplacian is assumed to be symmetric (i.e. even) the <script type="math/tex" id="MathJax-Element-454">\sin</script> terms vanish and we are left with something of the form <script type="math/tex" id="MathJax-Element-455">\sum f \cdot (1-\cos)</script> where the sum over ones comes from the main diagonal part of the Laplacian (which is s.t. every rowsum = 0, i.e. the sum over the other entries in the row)</li>
<li>on <script type="math/tex" id="MathJax-Element-456">\mathbb Z^d</script> in case of no “diagonal” contributions (i.e. <script type="math/tex" id="MathJax-Element-457">\Delta f(0,0)</script> is some function of <script type="math/tex" id="MathJax-Element-458">\{(x,y) : x=0 \;or\; y=0\}</script> one can reduce it to the problem on <script type="math/tex" id="MathJax-Element-459">\mathbb Z</script> by writing <script type="math/tex" id="MathJax-Element-460">\mathbb Z^d</script> as the Cartesian (graph) product of lower dimensional copies, see <a href="http://math.stackexchange.com/questions/139735/eigenstructure-of-discrete-laplacian-on-uniform-grid">http://math.stackexchange.com/questions/139735/eigenstructure-of-discrete-laplacian-on-uniform-grid</a></li>
<li>on general graphs (or <script type="math/tex" id="MathJax-Element-461">\mathbb Z^d</script> with “diagonal” contributions) it is better to interpret it as a convolution operator</li>
</ul>



<h1 id="laplace-transform">Laplace transform</h1>

<ul>
<li>superposition of moments → resolvent as a geometric series</li>
</ul>



<h1 id="why-is-red-greed-color-blindness-so-abundant">why is red greed color blindness so abundant?</h1>

<ul>
<li>since males are much more likely to have it (6% of males vs 0.4% of females), it goes without saying that the following is not the only/“primary” reason for it, but at least it helps to see why red&amp;green are the “problem colors” (as opposed to blue)</li>
<li>the mathematical setting: <br>
<ul><li>given some set <script type="math/tex" id="MathJax-Element-462">I</script> of “natural images” (arrays with RGB triples as entries) there are certain patterns <script type="math/tex" id="MathJax-Element-463">X</script> we absolutely have to recognize in order to survive (&amp;recreate)</li>
<li>letting the proportions <script type="math/tex" id="MathJax-Element-464">r,g,b</script> (w.r.t. <script type="math/tex" id="MathJax-Element-465">r+g+b=1</script>) of the cells responding to the respective colors vary we get an optimization problem <script type="math/tex" id="MathJax-Element-466">\min_{p,b}\sum_{i\in I, x\in X}f_{p,b}(i,x)=:\min_{p,b}F(p,b)</script>, where <script type="math/tex" id="MathJax-Element-467">f_{p,b}(i,x)</script> is a penalty for not recognizing some feature <script type="math/tex" id="MathJax-Element-468">x</script> in the image <script type="math/tex" id="MathJax-Element-469">i</script> given <script type="math/tex" id="MathJax-Element-470">b</script> cells for blue, <script type="math/tex" id="MathJax-Element-471">p</script> for red and <script type="math/tex" id="MathJax-Element-472">1-p-b</script> for green</li>
<li>surprisingly, (according to an informal discussion with Gasper Tkacik iirc) when you run some simulations, the energy landscape F (as a function of <script type="math/tex" id="MathJax-Element-473">p,b</script>) has a clear global minimum in terms of <script type="math/tex" id="MathJax-Element-474">b</script> (i.e. <script type="math/tex" id="MathJax-Element-475">dF/db</script> is big), but the derivative w.r.t. <script type="math/tex" id="MathJax-Element-476">p</script> almost vanishes - or in other words: in our world it seems to be very important to get the blue tones correct, but the rest is negligible (or at least it used to be before traffic lights became a thing)</li></ul></li>
</ul>

<!-- # random stopping times to get limits of n-th roots of random expressions (and more)&#10;* &#10;* for more fun stuff to do with randomized stopping times (given some time homogeneous Markov chain) see http://arxiv.org/pdf/1407.6831.pdf&#10;-->
