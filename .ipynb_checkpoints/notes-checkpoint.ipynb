{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering of noisy graphs + applications\n",
    "\n",
    "* https://arxiv.org/pdf/1702.00467.pdf an introduction\n",
    "* http://science.sciencemag.org/content/347/6224/1257601/tab-pdf \"Uncovering disease-disease relationships through the incomplete interactome\" - how to use methods from physics devised for studying noisy graphs and their phase transitions to classify diseases \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://www.macs.hw.ac.uk/~simonm/ae.pdf\n",
    "\n",
    "nice introduction to asymptotic series and how to apply them to Laplace integrals (p21 ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# on the σ-algebra induced by finite-dimensional distributions\n",
    "\n",
    "* let $(\\Omega,\\mathcal A,\\mathbb P)$ be a probability space\n",
    "* a stochastic process is a function $X:\\Omega\\times T\\rightarrow\\mathbb R$, s.t. $X(\\cdot,t)=:X(t)$ is a random variable for all $t\\in T$ (i.e. is $\\mathcal A-\\mathcal B_{\\mathbb R}$-measurable, where $\\mathcal B_{\\mathbb R}$ is the Borel σ-algebra on $\\mathbb R$)\n",
    "* using [Kolmogorov's extension theorem](https://en.wikipedia.org/wiki/Kolmogorov_extension_theorem) we see that to define a stochastic process $X$ on some possibly uncountable index set $T$ it suffices to specify its finite-dimensional distributions (which have to satisfy some consistency relations), i.e. viewing $X(\\cdot)$ as some random variable on $\\mathbb R^T$, it suffices to specify the probability of cylinder sets $\\prod_{t\\in T}A_t$, where only finitely many $A_t\\in\\mathcal B_{\\mathbb R} s.t. A_t\\neq\\mathbb R$\n",
    "* call the σ-algebra induced by those cylinder sets $\\mathcal C$, the \"cylindrical σ-algebra\"\n",
    "* it is often said (e.g., see the Wikipedia article on [Kolmogorov's extension theorem](https://en.wikipedia.org/wiki/Kolmogorov_extension_theorem)) that this algebra is not \"very rich\", but what does that mean?\n",
    "* see https://fabricebaudoin.wordpress.com/2012/03/24/lecture-2-measure-theory-in-function-spaces/, the first exercise which says that we cannot measure (the probability of) events like \"all functions from $T$ to $\\mathbb R$ of which the supremum is $\\leq 1$\"\n",
    "* instead we have to restrict ourselves to a smaller class of functions, e.g. continuous/cadlag ones. why we have to do that becomes apparent when trying to prove why events like above are not in $\\mathcal C$:\n",
    "  * take e.g. $T=[0,1]$ (but any other uncountable $T$ will do as well), then the above problem can be phrased as the question whether $\\prod_{x\\in T}(-\\infty,1]$ is in $\\mathcal C$\n",
    "  * remember that $\\mathcal C$, the σ-algebra generated by cylinder sets, is the smallest σ-algebra containing all those cylinder sets\n",
    "  * for a σ-algebra we only need complements and _countable_ unions (hence also only countable intersections) to be in the set, so we only get \"control over _countably_ many coordinates\", i.e. sets of the form $\\prod_{t\\in T'}(-\\infty,1]\\times\\prod_{t\\in T\\setminus T'}\\mathbb R$, where $T'\\subset T$ is countable\n",
    "* so to be able to measure \"global\" events like the set of all functions (in some given function space) having some property that depends on every single point we need to restrict ourselves to function spaces where we can control this behaviour by only controlling countably many points, e.g. continuous functions\n",
    "\n",
    "## the cylindrical σ-algebra coincides with the Borel σ-algebra for uniformly continuous $X$ on totally bounded $T$ generated by $\\|\\cdot\\|_\\infty$\n",
    "\n",
    "* let $X$ be s.t. it takes values in the space of $d$-uniformly continuous functions $C_u(T,d)=C_u$, where $d$ is some metric on $T$, where $T$ is totally bounded\n",
    "* obviously $C_u\\subset \\mathbb R^T$, so we can think of $X:\\Omega\\times T\\rightarrow\\mathbb R$ as element of $\\mathbb R^{\\Omega\\times T}=(\\mathbb R^T)^\\Omega$, so a function $X':\\Omega\\rightarrow \\mathbb R^T, \\omega\\mapsto X(\\omega,\\cdot)$ - **if this map was measurable we could identify $X$ with the *random variable* $X'$ taking values in the space of functions from $T$ to $\\mathbb R$**\n",
    "* first, to make sense of \"$X'$ being measurable\", one has to specify the σ-algebras on $\\Omega$ and $R^T$ though\n",
    "  * for $\\Omega$ we stick to the natural choice $\\mathcal A$\n",
    "  * endow $C_u$ with the (suitable restriction of the) cylindrical σ-algebra $\\mathcal C$ (restriction because $C_u\\subset \\mathbb R^T$, in general one cannot simply endow a subspace with the bigger space's σ-algebra - by abuse of notation let us simply call this $\\mathcal C$ again though)\n",
    "  * given some consistent family of finite-dimensional distributions we can use Kolmogorov's extension theorem like above to obtain a unique probability measure $\\mu:\\mathcal C\\rightarrow [0,1]$ which satisfies $\\mu(A)=\\mathbb P(X'^{-1}(A)) \\forall A\\in\\mathcal C$\n",
    "  * note that the coordinate functions $X_t:\\Omega\\rightarrow\\mathbb R$ are random variables, i.e. measurable, by definition of a stochastic process, so a priori we could only state the previous statement for finite $T$, but thanks to Kolmogorov's extension we actually get that $X'$ is $(\\Omega,\\mathcal A)-(C_u,\\mathcal C)$-measurable, i.e. a random variable\n",
    "  * (to see that note that for $\\mathbb P$ to be defined the preimage under $X'$ of every $A\\in\\mathcal C$ has to be in $\\mathcal A$, which is just the definition of $(\\Omega,\\mathcal A)-(C_u,\\mathcal C)$-measurable)\n",
    "* note that the space of continuous functions on a compact set endowed with $\\|\\cdot\\|_\\infty$ is a separable Banach space (Dudley - real analysis & probability, 11.2)\n",
    "* because of *uniform* continuity we can write $C_u(T,d)=C(\\overline{(T,d)})$, where $\\overline{(T,d)}$ is the completion of $T$ w.r.t. $d$, so it is a separable Banach space and we can find $T_0\\subset T$ countable s.t. $\\|f\\|_\\infty := \\sup_{t\\in T} |f(t)|=\\sup_{t\\in T_0}|f(t)|$\n",
    "* hence the closed balls $\\{f\\in C_u: \\|f-f_0\\|_\\infty\\leq r\\} = \\bigcap_{t\\in T_0}\\{f\\in C_u:|f(t)-f_0(t)|\\leq r\\}\\in \\mathcal C$, since $\\{f\\in C_u:|f(t)-f_0(t)|\\leq r\\}\\in \\mathcal C$ and $T_0$ countable \n",
    "* morever, in a seperable metric space every open set is a countable union of closed balls so all open sets are in $\\mathcal C$ and hence the Borel σ-algebra $B_{C_u(T,d),\\|\\cdot\\|_\\infty}$ (of $C_u(T,d)$ w.r.t. the topology generated by $\\|\\cdot\\|_\\infty$) is a subset of $\\mathcal C$\n",
    "* the converse inclusion ($\\mathcal C\\subset B_{C_u(T,d),\\|\\cdot\\|_\\infty}$) can easily be seen using arguments similar to the one with closed balls before\n",
    "* hence we have $\\mathcal C = B_{C_u(T,d),\\|\\cdot\\|_\\infty}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mathematics & brain activity\n",
    "\n",
    "## http://www.pnas.org/content/113/18/4909.abstract\n",
    "\n",
    "> mathematical judgments were related to an amplification of brain activity at sites that are activated by numbers and formulas in nonmathematicians, with a corresponding reduction in nearby face responses\n",
    "\n",
    "## http://journal.frontiersin.org/article/10.3389/fnhum.2014.00068/full\n",
    "> the experience of mathematical beauty correlates parametrically with activity in the same part of the emotional brain […] as the experience of beauty derived from other sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hölder's inequality to bound expressions of the form $\\prod_i \\mathbb E[|X|^{n_i}]$\n",
    "\n",
    " * let $0<n_i<m$ s.t. $\\sum_i n_i=m$ and define $p_i=\\frac{n_i}{m}, q_i=\\frac{p_i}{p_i-1}$\n",
    " * apply Hölder's inequality to $|X|^{n_i}$ and $1$ to get $\\mathbb E[|X|^{n_i}]\\leq \\mathbb E[|X|^m]^\\frac{n_i}{m}$\n",
    "   * note that taking $n_i$-th roots here yields monotonicity of $p$-norms.\n",
    " * multiplying this out gives $$\\prod_i \\mathbb E[|X|^{n_i}]\\leq \\mathbb E[|X|^m]$$\n",
    " * note that usually Hölder goes \"in the opposite direction\", i.e. bounding expectation of a product by a product of expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a blog about maths\n",
    "Given \"constraints\":\n",
    " 1. want a static site generator (in particular an automatically generated landing page with links (+summaries) to(/of) articles\n",
    " 1. hosting on Github (or something similar) with a reasonably easy way to update and upload new articles\n",
    " 1. want to use Markdown for text\n",
    " 1. want to use LaTeX syntax (not only LaTeX-like!) for formulas\n",
    " 1. it shouldn't look too bad (e.g. oversized formulae, etc.)\n",
    " \n",
    "<!--At first I tried Jekyll which excelled at 1,2,3, while falling short of fulfilling 4,5 properly (the reason being that Kramdown (but also all the other non-default Markdown parsers) was a pain when formulae including an underscore for indices appeared. Also the size of the formulae was pretty random (pretty sure one could work around that, but given the annoying first point I didn't bother).-->\n",
    "\n",
    "I found that IPython notebooks were perfect for parsing a combination of Markdown and LaTeX, parsing everything correctly and offering an export option.\n",
    "\n",
    "Potential options:\n",
    " * **Pelican.** (automatic) Works similarly like Jekyll (just a little bit more complicated in choosing&adjusting themes, editing metadata and previewing) and has a plugin for IPython support. [[Instructions how to set it up](https://www.dataquest.io/blog/how-to-setup-a-data-science-blog/)]\n",
    " * **Github.** (semi-automatic) If you don't mind their design just upload your blog posts as IPython files there and edit the README manually to get a \"landing page\".\n",
    " * **Manually.** (DIY) Just export IPython notebooks as HTML files and keep one HTML file updated as a landing page. May be a pain to work with, but at least you don't need hours of setting everything up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplacians in probability (TO BE WRITTEN)\n",
    "* [mixing times → graph mincut → Cheeger's constant] → find EFs of Laplacian\n",
    "* minimization of Dirichlet energy $\\mathcal E(f,g)=\\int \\nabla f\\nabla g$ → EFs of Laplacian (spectral gap), can be seen intuitively by partial integration or by an easy explicit calculation as in the mixing times notes\n",
    "* from those calculations we get a discrete $L^2$ and probabilistic analogue of Poincaré's inequality (interpret $\\|f\\|_2^2$ as $var(f)$), interesting since spectral gap gives the relation between local and global \"fluctuations\"\n",
    "* [relate spectral gap to mixing times → geometrical interpretation of the constant in Poincaré's inequality]\n",
    "* [inverse spectral problems like dimensionality reduction/manifold reconstruction]\n",
    "<!-- Belkin thesis p.17ff/20ff, https://en.wikipedia.org/wiki/Poincar%C3%A9_inequality -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [rotation+translation invariance → function of Laplacian](http://www.math.harvard.edu/~canzani/math253/Lecture1.pdf)\n",
    "* given an operator $S$ which is invariant under translations and rotations, i.e. $S(f\\circ \\tau) = (Sf)\\circ\\tau$, where $\\tau$ stands for translations/rotations\n",
    "* then $S$ is a function of the Laplacian, in particular in $\\mathbb R^d$ we have some $m$ and coefficients $\\{a_i\\}_i$ s.t. $S=\\sum_i^m a_iL^i$, where $L$ is the Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random stopping times to get limits of n-th roots of random expressions (and more)\n",
    "* http://peter.muehlbacher.me/math/2016/12/02/from-counting-trees-to-bounding-eigenvalues/\n",
    "* for more fun stuff to do with randomized stopping times (given some time homogeneous Markov chain) see http://arxiv.org/pdf/1407.6831.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# why is red green color blindness so abundant?\n",
    "* since males are much more likely to have it (6% of males vs 0.4% of females), it goes without saying that the following is not the only/“primary” reason for it, but at least it helps to see why red&green are the “problem colors” (as opposed to blue)\n",
    "* the mathematical setting:\n",
    "  * given some set $I$ of “natural images” (arrays with RGB triples as entries) there are certain patterns $X$ we absolutely have to recognize in order to survive (&recreate)\n",
    "  * letting the proportions $r,g,b$ (w.r.t. $r+g+b=1$) of the cells responding to the respective colors vary we get an optimization problem $\\min_{p,b}\\sum_{i\\in I, x\\in X}f_{p,b}(i,x)=:\\min_{p,b}F(p,b)$, where $f_{p,b}(i,x)$ is a penalty for not recognizing some feature $x$ in the image $i$ given $b$ cells for blue, $p$ for red and $1-p-b$ for green\n",
    "  * surprisingly, (according to an informal discussion with Gasper Tkacik iirc) when you run some simulations, the energy landscape F (as a function of $p,b$) has a clear global minimum in terms of $b$ (i.e. $dF/db$ is big), but the derivative w.r.t. $p$ almost vanishes - or in other words: in our world it seems to be very important to get the blue tones correct, but the rest is negligible (or at least it used to be before traffic lights became a thing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace transform\n",
    "* superposition of moments → resolvent as a geometric series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://www.cs.columbia.edu/cg/pdfs/1180993110-laplacian.pdf\n",
    "* paper discussing different variants of discrete Laplacians and their properties like symmetry, positivity, positive definiteness, locality and maximum principles\n",
    "* main result: there are graphs that do not admit all those properties simultaneously (argues with dual graphs)\n",
    "\n",
    "# eigenvalues of the discrete Laplacian (or generally translation invariant and symmetric operators)\n",
    "* 1D case: writing out the Laplacian in the standard basis as a matrix it is easy to see that it is a circulant matrix and thus diagonalised by the discrete Fourier transform\n",
    "* general case (take 1): see http://mathoverflow.net/a/2829/47059, which basically says that the eigenvectors(/functions) of the Fourier transform (exp) also happen to be eigenvectors(/functions) of the translation operator → diagonalize with DFT to get eigenvalues\n",
    "* (slightly less) general case (take 2): write the action of the (discrete) Laplacian on some vector as a convolution → we know that the Fourier transformation turns convolutions into multiplications → integral operator turns into a multiplication operator, i.e. is diagonalized\n",
    "* more conrete results on $\\mathbb Z$: from general theory about circulant matrices we know that the eigenvalues are just the Fourier transform of one row(/column since it’s symmetric) - since the Laplacian is assumed to be symmetric (i.e. even) the $\\sin$ terms vanish and we are left with something of the form $\\sum f \\cdot (1-\\cos)$ where the sum over ones comes from the main diagonal part of the Laplacian (which is s.t. every rowsum = 0, i.e. the sum over the other entries in the row)\n",
    "* on $\\mathbb Z^d$ in case of no “diagonal” contributions (i.e. $\\Delta f(0,0)$ is some function of $\\{(x,y) : x=0 \\;or\\; y=0\\}$ one can reduce it to the problem on $\\mathbb Z$ by writing $\\mathbb Z^d$ as the Cartesian (graph) product of lower dimensional copies, see http://math.stackexchange.com/questions/139735/eigenstructure-of-discrete-laplacian-on-uniform-grid\n",
    "* on general graphs (or $\\mathbb Z^d$ with “diagonal” contributions) it is better to interpret it as a convolution operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic results from extreme value theory\n",
    "* $\\sup_{n\\leq k} X_n ~ \\sigma\\sqrt{2\\ln k}$, $X_n$ being Gaussian with variance \\sigma\n",
    "* fluctuations are of inverse order $(\\sigma\\sqrt{2\\ln k})^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# existence of LDPs by subadditivity\n",
    "* assume $\\pi_n$ is some sequence of events we want to get a LDP for, e.g. $\\pi_n = P(S_n\\geq nu)$ for some fixed $u$ and $S_n$ being some sum of random variables\n",
    "* if we can show that $\\pi_{n+m}\\geq\\pi_n \\pi_m$ (subadditivity - check this e.g. for sum of independent r.v.), we can transform them to $p_n := -ln \\pi_n$ satisfying $p_{n+m}\\leq p_n p_m$ and use Fekete’s subadditive lemma ($a_n\\geq 0$ subadditive → $\\lim_{n\\rightarrow\\infty}a_n/n = \\inf_{n\\geq 1} a_n/n$ to show that the LDP exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# when are Chernoff bounds sharp?\n",
    "* Chernoff-type bounds are something of the form $P(X\\geq a)\\leq\\mathbb E[e^{tX}]/e^{ta}\\forall t>0$\n",
    "* this can easily be seen by writing $P(X\\geq a)\\leq P(e^{tX}\\geq e^{ta})$ and applying Markov’s inequality\n",
    "* a geometric interpretation:\n",
    "  * write the probability of some event $A$ as expectation of its indicator function $\\bf 1_A$\n",
    "  * bounding the probability of $X\\geq a$ with $e^{tX}\\geq e^{ta}$ should be thought of as some exponential function touching the outer left part of the indicator function - which is, a priori, obviously not a bound that one expects to be sharp\n",
    "* in the proof of Cramer’s theorem (LDP), however, we only look at random variables that are ``exponentially’’ concentrated around their mean (that’s why we need large deviations after all), so the intuition is that the Chernoff bound is sharp since the whole mass is concentrated just around the point where the exponential function touches the indicator function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geometric interpretation of the (conditional) expectation\n",
    "* introduce conditional expectation $\\mathbb E[X|\\mathcal G]$ as follows:\n",
    "  * let $X$ be an integrable random variable on some probability space $(\\Omega,\\mathcal A, P)$ \n",
    "  * let $\\mathcal G\\subset\\mathcal A$ be a sub-$\\sigma$-algebra\n",
    "  * conditional expectation of $X$ given $\\mathcal G$, $\\mathbb E[X|\\mathcal G]$ is *any* random variable $Y$ s.t.\n",
    "    * $Y$ is $\\mathcal G$-measurable\n",
    "    * $\\forall A\\in\\mathcal G: \\mathbb E[X\\bf 1_A]=\\mathbb E[Y\\bf 1_A]$\n",
    "* it can be seen that such a $Y$ exists and, up to $P$-null equivalence, is uniquely determined by those conditions\n",
    "* moreover we have all the \"typical\" properties of expectations like\n",
    "  * positivity: $X\\geq 0$ implies $\\mathbb E[X|\\mathcal G]\\geq 0$ $P$-a.s.\n",
    "  * linearity\n",
    "  * monotonicity\n",
    "  * Jensen’s inequality: $\\phi(\\mathbb E[X|\\mathcal G])\\leq\\mathbb E[\\phi(X)|\\mathcal G]$, $P$-a.s. for $\\phi$ convex and $X$ s.t. $\\mathbb E|X|<\\infty$, $\\mathbb E|\\phi(X)|<\\infty$\n",
    "  * importantly: $X$ being $\\mathcal G$-measurable, ($\\mathbb E|X|,\\mathbb E|Y|<\\infty$) implies $\\mathbb E[XY|\\mathcal G]=X\\mathbb E[Y|\\mathcal G]$, $P$-a.s., which, after identifying the deterministic ``random’’ variable $X(\\omega)=1 \\forall\\omega$ with $1\\in\\mathbb R$, gives the second axiom for an expectation in terms of free probability, i.e. $\\mathbb E[1|\\mathcal G]=1$ (the first one being linearity)\n",
    "* one interpretation of this conditional expectation is a geometric one which can be made precise under suitable technical assumptions ($X\\in L^2(\\Omega,\\mathcal A,P)$): $\\mathbb E[X\\mathcal G]$ is the orthogonal projection of $X$ from $L^2(\\Omega,\\mathcal A,P)$ on $L^2(\\Omega,\\mathcal G,P)$\n",
    "* to put this in words: start with some random variable (/function of degrees of freedom determined by $\\mathcal A$) $X$, then find the function $\\tilde X$ in the space of functions with fewer degrees of freedom (i.e. those given by $\\mathcal G$ which has to be a subset of $\\mathcal A$) that is ``closest’’ to the original one\n",
    "* to illustrate what is meant by ``degrees of freedom’’ it is instructive to look at the example $\\mathcal G=\\{\\emptyset,\\Omega\\}$; to be $\\mathcal G$ measurable now means that the pre-image of every open set in the domain of $X$ has to be an element of $\\mathcal G$, which, for this $\\mathcal G$, implies that there are only constant functions - the second property in the definition now gives that this constant value has to be the classical expectation\n",
    "* conversely, if we take a bigger $\\sigma$-algebra, we give ourselves more degrees of freedom for $\\tilde X$ and can, in a sense stay closer to the original $X$, which, from a probabilistic point of view, just means that more randomness is retained - so if you think of some filtration, encoding e.g. the ``knowledge’’ after flipping a coin for the $n$-th time, and the random variables $S_n=X_1+\\dots+X_n$ being the sum of all outcomes up to the $n$-th flip, conditioning on the first element of the filtration (i.e. the $\\sigma$-algebra $\\mathcal G_1=\\sigma(\\{(0,\\{0,1\\},\\{0,1\\},\\dots),(1,\\{0,1\\},\\{0,1\\},\\dots)\\})$) just means that $X_1$ is measurable, so $\\mathbb E[X_1+X_2+\\dots+X_n|\\mathcal G_1]=\\mathbb E[X_1|\\mathcal G_1]+\\mathbb E[X_2+\\dots+X_n|\\mathcal G_1]$ by linearity and $E[X_1|\\mathcal G_1]$ simplifies to $X_1$, whereas $\\mathbb E[X_2+\\dots+X_n|\\mathcal G_1]$ simply becomes the usual expectation $1/2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp of involutions\n",
    "* for any involution $a:a^2=I$, where $I$ is the identity, one has $exp(ca)=cosh(c)+a sinh(c)$ for scalar $c$, which can be seen by looking at the power series\n",
    "* can also be used for sin/cos with exp=i*sin+cos\n",
    "* can also be written as $cosh(c)(1+a tanh(c))$ which can be seen as “low temperature expansion” (think about $c$ as some temperature close to zero or more mathematically as some small $\\varepsilon>0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to get compact sets in non Euclidean settings\n",
    "* (one) idea: given some compact inclusion $i$ of space $X$ into $Y$, find some bounded set $B\\subset X$ and consider $i(B)$\n",
    "* this is useful for locally convex spaces of all sorts and works well with machinery like the Rellich–Kondrachov theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# marking direct objects with 把 (中文)\n",
    "* used like [subject]把\\[object\\][transitive verb]\n",
    "* e.g. 他把我的杯子打破了｡ (He broke my cup.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\{x : |x|_i<\\varepsilon, i\\in I\\}$ does not have to be an open set\n",
    "* take product topology (cylinder sets $\\{x : |x|_i<\\varepsilon, i\\in J\\}$ for $J$ being a finite subset of $I$ are a neighbourhood basis of $0$)\n",
    "* if $I$ is not finite, no element in the neighbourhood basis is contained in the set in the heading (if the norms $|\\cdot|_i$ do not trivially depend on each other) → it is not an open set\n",
    "* examples: locally convex spaces in PDEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# showing emphasis with 是…的 (中文)\n",
    "* encapsulate the part to be stressed with 是…的, e.g. 我(是)來台北找朋友的｡ (I came *to Taipei* to visit a friend.)\n",
    "* in positive sentences 是 may be omitted\n",
    "* only for events in the past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# green, white, black and Oolong tea\n",
    "http://cooking.stackexchange.com/questions/26002/what-is-the-difference-between-green-white-and-black-tea for quick introduction (also Oolong tea)\n",
    "\n",
    "* fermentation: green&white tea leaves are quickly heated (typically steamed in Japan, while pan-fired in China) after harvesting to reduce oxidation\n",
    "* white tea\n",
    "  * minimally processed to leave downy hair intact\n",
    "  * not oxidized → does not develop as much flavor, color, or caffeine\n",
    "* green tea\n",
    "  * history\n",
    "    * starting from 17th century (Europeans started importing tea) until mid 19th century green tea was about as popular as black tea (in Europe)\n",
    "    * originally (2000 bc) used as medicine in China\n",
    "    * in the 8th century 陸羽 (Lù Yǔ) wrote 茶經 (chájīng), the first known monograph on tea where he described tea being steamed and formed into tea bricks for storage and trade - to prepare tea one had to pulverize it before brewing → 抹茶 (matcha)\n",
    "    * only later it was established\n",
    "  * rolling wilted leaves breaking cell walls to speed release of aromatic substances\n",
    "* types of tea\n",
    "  * China\n",
    "    * 珠茶 (gunpowder): rolled into little round pellets (green or Oolong tea); shiny pellets indicate freshness, little pellets are considered a mark of higher quality tea\n",
    "    * 龙井茶 (Lóng Jǐng tea): renowned for its high quality\n",
    "    * 黄山毛峰 ((Huángshān) Máo Fēng): mild-flavored, very popular tea\n",
    "    * 茉莉花茶 (Jasmine tea): subtly sweet and highly fragrant, stored with blossoms to acquire their scent\n",
    "    * 白牡丹 (Bái mǔdān): one of the most well-known white teas\n",
    "  * Japan\n",
    "    * 煎茶 (Sencha): most popular tea in Japan (80°C, 1 min, 1.5 tablespoons (7-8 grams) per litre)\n",
    "    * 茎茶 (Kukicha): blend made of stems, stalks, and twigs (80°C, 40sec to 1min, 4 teaspoons per litre)\n",
    "    * 抹茶 (Matcha): mentioned above\n",
    "    * 玉露 (Gyokuro): grown under the shade rather than the full sun since the more sun, the more Catechin (bitter)\n",
    "* aromatized tea: additional flavors, but basically loses its ability of being brewed more than once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://arxiv.org/pdf/1411.1792v1.pdf\n",
    "first few layers of CNNs with similar purposes tend to be very similar (Gabor filters) → can copy them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://arxiv.org/pdf/1301.3583v4.pdf\n",
    "increasing the size of neural networks only yields diminishing returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://arxiv.org/pdf/1312.5851v5.pdf\n",
    "how to implement the convolutions of CNNs with the FFT (feasible for large number of feature maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://math.stackexchange.com/questions/766479/what-is-spectrum-for-laplacian-in-mathbbrn, http://math.stackexchange.com/questions/790401/spectrum-of-laplace-operator\n",
    "two great articles covering why the spectrum of the Laplace operator (on $\\mathbb R^N$) is $(-\\infty,0]$ which, by looking more closely, also explains why “unbounded domain” implies “uncountable basis of eigenfunctions”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://cs231n.github.io/convolutional-networks/\n",
    "great introduction to convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://stackoverflow.com/questions/7536465/create-a-2d-array-with-a-nested-loop\n",
    "why you want to use `[[None for j in xrange(3)] for i in xrange(3)]` instead of `[[None]*3]*3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://arxiv.org/pdf/1405.4537v1.pdf (TO BE WRITTEN)\n",
    "* def: stream is a map $\\gamma$ from a totally ordered set $I$ to some state space\n",
    "* there is a canonical way to convert discrete streams to continuous paths (path: $I$ interval and some regularity conditions like right continuity)\n",
    "* from now on “wlog”: $\\gamma:[J_-,J_+]\\rightarrow E$ will continuously map an interval $J$ to some Banach space $E$\n",
    "* def: bounded $p$-variation ($p\\geq 1$) iff $sup_{\\dots<u_i < u_{i+1}<\\dots\\in J}\\sum_i\\|\\gamma_{u_{i+1}}-\\gamma_{u_i}\\|^q <\\infty$ for $q=1$ and $q=p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFT\n",
    "* DFT in $N$ dimensions can be written as $F_N=1/\\sqrt N (\\omega_N^{kl})_{k,l=0}^{N-1}$, where $\\omega_N=e^{2\\pi i/N}$ and this matrix is a Vandermonde matrix over the roots of unity\n",
    "* multidimensional DFT consists of iterated sums which commute → can write it as $F_{N_2}(F_{N_1}(X_{i,j})_{j=0}^{N_1-1})_{j=0}^{N_2-1}$\n",
    "* Vandermonde matrix = evaluation of a polynomial at points generating the Vandermonde matrix → upsampling = ($e^{2πinm/2N})_{n,m=0}^{n=N-1,m=2N-1}$ (Vandermonde matrix with twice as many rows as a square one → evaluates the polynomial described by the given vector at twice as many points of the unit circle (→ to convert a signal of frequency a to one with frequency b you first need to upsample it to LCM(a,b))\n",
    "* real signal x → DFT(x) has entries that are complex conjugated to each other (usually mirrored around half the length of the signal) since they are just the $L^2(\\mathbb C)$ inner product $(\\langle x,e_k\\rangle)_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linearisation of a differential operator\n",
    "http://math.stackexchange.com/questions/1677181/how-do-you-linearize-a-differential-operator-to-get-its-symbol for an example\n",
    "\n",
    "* used (i.a.) to define the symbol of a differential operator $D$\n",
    "* how to do it:\n",
    "  * usually linearize around a solution $\\tilde u, D\\tilde u=0$\n",
    "  * idea is to look at how $D$ behaves on functions $u$ that are close to the point one is linearising around (i.e. $\\tilde u$), so one does the substitution $u=\\tilde u+\\varepsilon v$\n",
    "  * rearranging terms like a power series in $\\varepsilon$ and using that $D\\tilde u=0$ one gets equations with $\\tilde u$ and $v$ which define an operator $A_{\\tilde u}$ acting on $v$, which satisfies $A_{\\tilde u}v = 0$ and is linear!\n",
    "* note that the linearisation depends on the solution just as the linearisation of a scalar valued function depends on the point $x_0$ one wants to linearize around and that the origin shifts accordingly, i.e. the input $v$ is much like $x-x_0$ and not $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://www-etud.iro.umontreal.ca/~sordonia/pdf/sigir2013_sordoni.pdf (TO BE WRITTEN)\n",
    "* problem: model dependencies of words\n",
    "  * either: need additional features (“computer”, “architecture” and “computer architecture” are completely different entries)\n",
    "  * or: model it as joint probabilities (less improvements than expected, huge computational effort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://math.ucr.edu/home/baez/rosetta.pdf\n",
    "great paper for everybody who is interested in (yet not familiar with) category theory - summary may follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# uses of 的 (中文)\n",
    "* possessive particle: 我的女 = my daughter\n",
    "* attributive (connecting adjective and noun): 红色的菜 = red vegetables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://arxiv.org/pdf/cond-mat/0611023v1.pdf\n",
    "* distribution of eigenvalues of the Hessian of a critical point is a shifted semicircle\n",
    "* e.g. global minimum → left of SCL is at 0, the bigger its energy, the more it is shifted to the left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# asking yes/no questions (中文)\n",
    "* there are two ways of doing so, either with\n",
    "  * 吗: e.g. 吃飽了嗎？ (have you eaten?), or\n",
    "  * X不X: 你要不要去北京？ (Do you or don’t you want to go to Beijing?) which is equivalent to 你要去北京嗎？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# past tense: 了 vs 过 (中文)\n",
    "* 过: applicable if action is repeatable and is finished\n",
    "* 了: applicable for events which started in the past and continue to the present\n",
    "* e.g. 她去过美国｡vs 她去美国了｡ (过→she is not there anymore, 了→she is still there)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# expressing prior/posterior actions with 以前/以后 (中文)\n",
    "* as “previously”/“after”:\n",
    "  * 他以前住在美国｡ (He (previously) lived in America.)\n",
    "  * 他以后会去美国｡ or 以后他会去美国｡ (He will go to America (afterwards).) — note that 以后 can be before or after the subject!\n",
    "* relatively prior to/after something else:\n",
    "  * [action]以前[prior action], e.g. 他睡觉以前喜欢看书｡(He likes to read before going to bed. (literally – He sleep before, likes read.))\n",
    "  * [action]以后[later action], e.g. 他下课以后要回家吃饭｡(After class, he will return home to eat. (literally – He class is over after, going to return home eat a meal.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Braess’ paradox](http://ist.ac.at/fileadmin/user_upload/pdfs/Talks/2016/02/Talk_Timme.pdf)\n",
    "* the paradox: given some (directed) graph with a flow (e.g. traffic network) removing edges could improve the “overall situation” (e.g. on average you do not drive as long as before if a street is closed)\n",
    "* note that this also implies the converse: adding a street doesn’t necessarily make the traffic situation better overall\n",
    "* one take on this paradox (from a dynamical systems point of view): knowing the capacity of all the edges and the nodes one gets a system of constraints for every closed loop in order to satisfy some stability condition, but if one introduces another edge that divides a circle in two we get two systems of constraints that may not be compatible → no more stable solution (= traffic jam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://www.huffingtonpost.com/2015/05/13/andrew-ng_n_7267682.html\n",
    "* “The idea is that innovation is not these random unpredictable acts of genius, but that instead one can be very systematic in creating things that have never been created before. […] learn a lot, read a lot, talk to experts.”\n",
    "* on early influences: moved around, visited many different colleges and interned at different labs → many different points of view\n",
    "* don’t “follow your passion” (which usually gets amended to “follow your passion of all the things that happen to be a major at the university you’re attending”); often “you first become good at something, and then you become passionate about it. And I think most people can become good at almost anything. So when I think about what to do with my own life, what I want to work on, I look at *two criteria*. The first is whether it’s an *opportunity to learn*. Does the work on this project allow me to learn new and interesting and useful things? The second is the *potential impact*. **The world has an infinite supply of interesting problems. The world also has an infinite supply of important problems. I would love for people to focus on the latter.**”\n",
    "* “one pattern of mistakes I’ve made in the past, hopefully much less now, is doing projects where you do step one, you do step two, you do step three, and then you realize that step four has been impossible all along”\n",
    "* “if you seriously study half a dozen papers a week and you do that for two years, after those two years you will have learned a lot. […] But that sort of investment, if you spend a whole Saturday studying rather than watching TV, there’s no one there to pat you on the back or tell you you did a good job. Chances are what you learned studying all Saturday won’t make you that much better at your job the following Monday. There are very few, almost no short-term rewards for these things. But it’s a fantastic long-term investment. […] People that count on willpower to do these things, it almost never works because willpower peters out. Instead I think people that are into creating habits — you know, studying every week, working hard every week — those are the most important. Those are the people most likely to succeed.”\n",
    "* “I don’t work on preventing AI from turning evil for the same reason that I don’t work on combating overpopulation on the planet Mars”, more urgent: “[…] when the U.S. transformed from an agricultural to a manufacturing and services economy, we had people move from one routine task, such as farming, to a different routine task, such as manufacturing or working call service centers. A large fraction of the population has made that transition, so they’ve been okay, they’ve found other jobs. But many of their jobs are still routine and repetitive. The challenge that faces us is to find a way to scalably teach people to do non-routine non-repetitive work. Our education system, historically, has not been good at doing that at scale. The top universities are good at doing that for a relatively modest fraction of the population. But a lot of our population ends up doing work that is important but also routine and repetitive. That’s a challenge that faces our educational system.”\n",
    "* “One thing about speech recognition: most people don’t understand the difference between 95 and 99 percent accurate. Ninety-five percent means you get one-in-20 words wrong. That’s just annoying, it’s painful to go back and correct it on your cell phone. Ninety-nine percent is game changing. If there’s 99 percent, it becomes reliable.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://www.paulgraham.com/hs.html\n",
    "great text, you should read it!\n",
    "\n",
    "* if you don’t have a clear goal (which is usually the case) work forward from promising situations instead\n",
    "* if you have to choose, take those options that will give you the most promising range of options afterward\n",
    "* work on hard problems (“Writing novels is hard. Reading novels isn’t. Hard means worry: if you’re not worrying that something you’re making will come out badly, or that you won’t be able to understand something you’re studying, then it isn’t hard enough.”) → also get to know interesting people and get big ideas\n",
    "* “Put in time how and on what? Just pick a project that seems interesting: to master some chunk of material, or to make something, or to answer some question. Choose a project that will take less than a month, and make it something you have the means to finish. Do something hard enough to stretch you, but only just, especially at first. If you're deciding between two projects, choose whichever seems most fun. If one blows up in your face, start another. Repeat till, like an internal combustion engine, the process becomes self-sustaining, and each project generates the next one.”\n",
    "* “Don’t disregard unseemly motivations. One of the most powerful is the desire to be better than other people at something.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://terrytao.wordpress.com/2008/08/07/on-time-management/\n",
    "* “Another thing is that my ability to do any serious mathematics fluctuates greatly from day to day; sometimes I can think hard on a problem for an hour, other times I feel ready to type up the full details of a sketch that I or my coauthors already wrote, and other times I only feel qualified to respond to email and do errands, or just to take a walk or even a nap. I find it very helpful to organise my time to match this fluctuation: for instance, if I have a free afternoon, and feel inspired to do so, I might close my office door, shut off the internet, and begin typing on a languishing paper; or if not, I go and work on a week’s worth of email, referee a paper, write a blog article, or whatever else seems suited to my current levels of energy and enthusiasm. It is fortunate in mathematics that a large fraction of one’s work (with the notable exception of teaching, which one then has to build one’s schedule around) can be flexibly moved from one time slot to another in this manner. [A corollary to this is that one should deal with tasks before they become so urgent that they have to be done immediately, thus disrupting one’s time flexibility.]”\n",
    "* “A half-hearted system is probably worse than no system at all. A corollary to this is not to try to make an overly ambitious system ab nihilo that one is unlikely to follow faithfully; it is probably better to let such systems evolve over time.“\n",
    "* “Sometimes one should abandon one’s own rules and allow for serendipity. There have been many times, for instance, when I had planned to work on something during my lunch hour (grabbing something quick to eat), when I was interrupted by a colleague or visitor to go out to eat. It has often happened that I got a lot more out of that lunch (mathematically or otherwise) than I would have back at the office, though not in the way I would have anticipated.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://sivers.org/book/FluentForever\n",
    "first 625: http://fluent-forever.com/wp-content/uploads/2014/05/625-List-Thematic.pdf\n",
    "\n",
    "* start off with “minimal pair testing” (e.g. having to differentiate sounds like niece and knees)\n",
    "* use Google images, not translations\n",
    "* to memorize genders(/tones/…) imagine the masculine terms exploding, feminine catching fire, neuter shattering like glass (similar for tone colors)\n",
    "* good at remembering when images are violent/sexual/funny\n",
    "* cloze card types for functional words like “of”, “what”, …\n",
    "* for 10 ways to form plural, pick 10 nouns and use the person-action-object system (Tiger essen Fleisch)\n",
    "* also use this to learn verb/noun/adjective/adverb patterns\n",
    "* submit sentences to lang8 and put corrections in flash cards\n",
    "* TV series are easier than films (read ahead on Wikipedia, no subs)\n",
    "* card types should also ask for mnemonic (e.g. “What’s a phrase that includes [word]?”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://arxiv.org/pdf/1511.06444.pdf\n",
    "universality in halting time for spin glasses and deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://arxiv.org/pdf/1301.3537.pdf\n",
    "super short abstract:\n",
    "\n",
    "* pooling operators are for local invariance\n",
    "* 1 layer of convolutions is for learning 1 one-parameter-group invariance (or rather: convolutions leave already learned group invariance untouched and the training learns the invariance)\n",
    "* deep networks are basically doing group factorisation that’s stable w.r.t. perturbations of the group\n",
    "\n",
    "my notes:\n",
    "\n",
    "* the problem: find signal representation $\\Phi$ (e.g. classifier) that is\n",
    "\t* invariant under some transformation group $G$ (e.g. rotation), i.e. $\\Phi(x)=\\Phi(gx)$ for all $g\\in G$ and\n",
    "\t* is “stable” to perturbations (i.e. transformations $h$ that are “close” to the transformation group $G$ which can be thought of as a low dimensional manifold), i.e. $||\\Phi(\\varphi(h,x))-\\Phi(x)||\\leq C||x||d(h,G)$\n",
    "* approach: take convolutional networks and think of them in terms of signal processing, i.e.\n",
    "  * inputs $x\\in X$ = signals $x\\in L^2(\\Omega)$\n",
    "  * convolution kernels = filter bank $\\{\\psi_\\lambda\\}_\\lambda, \\lambda\\in\\Lambda_1$, e.g. for a filter bank corresponding to an expansion in a funtions Fourier series, convolution with $\\psi_\\lambda$ yields $\\langle x,e^{2\\pi i\\lambda/N}\\rangle$\n",
    "  * the first layer maps $x(\\cdot)\\mapsto \\{x*\\psi_\\lambda(\\cdot)\\}_\\lambda =: z^{(1)}(\\cdot,\\lambda)$ (which is the set of **convolutions** with kernels $\\psi_\\lambda$, applies some operator $M$ (the **activation function** which acts piece-wise) and then applies some **pooling operator** $P$, which, in terms of signal processing, can be thought of as low-pass filter (low-pass because chosing max-pooling can be thought of as eliminating high frequency oscillations), followed by downsampling (see Wikipedia for a quick explanation)\n",
    "* the special case of one-parameter transformation groups $G=\\{U_t\\}_{t\\in\\mathbb R}, U_t\\in L^2(\\Omega), \\lim U_t z = U_{t_0}z, U_{t+s}=U_tU_s$ (e.g. translations, frequency transpositions, dilations):\n",
    "  * we want to find a canonical way of describing the action of a group element of a one-parameter family; this is given by Stone’s theorem which (under reasonable assumptions) states that there is some s.a. $A$ such that $U_t=\\exp(itA)$ (spectral theorem!)\n",
    "  * since $A$ is self adjoint there is a change of basis given by $O$ that diagonalizes $A$, i.e. (in the finite dimensional case) $OAO^{-1} = \\text{diag}(\\lambda_1,\\dots,\\lambda_n)$\n",
    "  * hence we can write the group action $U_tz = O^{-1}(\\exp(it\\text{diag}(\\lambda_1,\\dots,\\lambda_n))Oz)$, implying that the group action is a linear phase change in the basis that diagonalizes $A$\n",
    "  * choosing $M$ to take the complex modulus yields a representation that is invariant under the action of $\\{U_t\\}_t$\n",
    "* “defining” perturbations of group elements:\n",
    "  * using the change of basis given by $O$ we can write the group action as some linear phase change; if we also apply the inverse Fourier transform this becomes a translation operator $T_s:z(\\cdot)\\mapsto z(\\cdot -s)$, which lets us measure deformations ($\\tilde T_s: z(\\cdot)\\mapsto z(\\cdot -\\tau(s))$ such that $d(\\tilde T_s,T_s)\\ll 1$) in a convenient way by analyzing the regularity of $\\tau$\n",
    "<!--  * thus the key to obtaining group invariant representations is not to find die eigenvectors of $A$ (yielding the change of basis given by $O$), but rather measurements that are “close” to diagonalising $A$ and are localised where deformations occur(?) - this can be done with convolutions with compactly supported filters-->\n",
    "I do not quite understand how this motivates the use of filters(/kernels) acting locally for group factorisation, but as soon as I do this will be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of the Fourier transform\n",
    "* diagonalises the derivative (i.e. turns it into a multiplication operator if expressed in its basis), which in turn let’s us define pseudo-differential operators via non-polynomial symbols\n",
    "* Fourier transform is like a projection onto eigenspaces of Laplace operator → in a compact domain (with suitable regularity) those are discrete → get a sum = Fourier _series_\n",
    "\t* using the above idea one can generalize Fourier transforms not only to higher dimensional setting and manifolds, but also to graph settings (see graph Laplacian)\n",
    "* translation = phase change (think of a shift in 1D and what this does to the coefficients of a Fourier _series_)\n",
    "  * in a space-time metric (having signature 1,1,1,-1) we can “place” a particle at some point in space-time by multiplying the creation operator with some exponential in the right basis (i.e. the one of the Fourier transform) where the spatial part has the inverse sign of the time part because of this metric\n",
    "* smoothness of the original function corresponds to decay in the Fourier domain (and vice versa), which can be made precise by the following statement about a random variable’s characteristic function (which is pretty much it’s Fourier transform)\n",
    "  * $\\mu({x: |x|\\geq 2/u})≤\\frac{1}{u} \\int_{-u}^u (1-\\hat \\mu(t))dt$\n",
    "  * in particular gives the following implication (in the appropriate setting): “characteristic function continuous in 0” → “measure is tight” (where tightness is useful since it enables application of Helly’s selection theorem, which is more or less the Banach-Alaoglu theorem for probability measures (latter would give weak-* convergence for signed, unnormed measures)\n",
    "* https://terrytao.wordpress.com/2009/04/06/the-fourier-transform/ for a more general take on the subject"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
