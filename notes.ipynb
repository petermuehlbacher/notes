{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explain [Choquet simplex](https://www.encyclopediaofmath.org/index.php/Choquet_simplex)\n",
    "\n",
    "* https://www.encyclopediaofmath.org/index.php/Choquet_simplex\n",
    "* https://en.wikipedia.org/wiki/Dual_space#Continuous_dual_space\n",
    "* https://en.wikipedia.org/wiki/Locally_convex_topological_vector_space\n",
    "* https://en.wikipedia.org/wiki/Riesz%E2%80%93Markov%E2%80%93Kakutani_representation_theorem\n",
    "* https://en.wikipedia.org/wiki/Positive_linear_functional (note characterisation for C\\* algebras with constant 1)\n",
    "* https://en.wikipedia.org/wiki/Hahn%E2%80%93Banach_theorem\n",
    "\n",
    "\n",
    "## Reminder: Simplex\n",
    "Def: Convex hull of $k$ affinely independent points(/\"vertices\").\n",
    "\n",
    "Ex: Triangle is, square is not.\n",
    "\n",
    "Note: Every point in a simplex _can_ (as it's a convex hull) be written _uniquely_ (by independence assumption) as an affine combination (i.e. factors sum to 1) of its vertices.\n",
    "\n",
    "Equivalent characterisation: When scaling and translating two copies (independently) of $K$, their intersection has to be a scaled & translated copy of $K$ again. (https://pdfs.semanticscholar.org/4b9d/3f9efdf03efaf7bac460e421066df87e881e.pdf THE CHOQUET REPRESENTATION IN THE COMPLEX CASE page 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Generalising the notion of a simplex to infinite dimensions\n",
    "\n",
    "\n",
    "\n",
    "will need: https://math.stackexchange.com/questions/875700/local-convexity-of-the-topology-of-weak-convergence-of-probability-measures for relating p.m. to locally convex spaces\n",
    "https://math.stackexchange.com/questions/1331321/cx-is-separable-when-x-is-compact for proof of Choquet\n",
    "https://math.stackexchange.com/questions/2579456/dual-space-x-separates-points-of-x how locally convex -> dual separates points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peter Mörter's trick (slightly generalized)\n",
    "\n",
    "given something of the form $\\prod_i(\\sum_k a_{k,i})$ can always be rewritten as $\\mathbb E[\\prod_i(A_i)]$, where $A_i$ is a random variable, picking any $a_{k,i}$ uniformly at random\n",
    "\n",
    "works particularly well if the individual $a_{k,i}$ behave nicely when multiplied (by their $i$ index), e.g. $a_{k,i} = e^{f(k)g(i)}$\n",
    "\n",
    "e.g. $\\prod_i \\cosh(Y_i)$, which doesn't admit a nice(r) formula at first sight becomes $\\mathbb E[e^{\\sum_i \\varepsilon Y_i}]$ with $\\varepsilon$ being $\\pm 1$ with equal probability; this is more amenable for taking derivatives for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLR condition ↔ Kolmogorov extension\n",
    "write it up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# local convexity (+Hausdorff?) → dual separates points\n",
    "write it up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [connected components in a graph ↔ branching processes](https://math.stackexchange.com/questions/2209475/branching-process-interpretation-of-giant-component)\n",
    "\n",
    "yet to summarise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [how to derive expected waiting times for a sequence of events (e.g. until HH or HT)](https://math.stackexchange.com/questions/521130/expected-value-of-flips-until-ht-consecutively)\n",
    "\n",
    "yet to summarise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# universality in statistics\n",
    "\n",
    "* **law of large numbers**\n",
    "* sum of \"sufficiently iid\" random variables is **Gaussian** (CLT); as a \"corollary\": **Brownian motion**.\n",
    "* product of \"sufficiently iid\" random variables satisfies **Benford's law**; this is definitely less well-known than the previous point, but should hold under even more relaxed assumptions since we only need the logarithm of the product to be \"roughly uniformly distributed\" over some sufficiently large range (depending on the base you use; usually base 10 is used, so the $\\log_{10}$ of the product should be roughly uniformly distributed in some connected interval containing multiple integers); note that in practice \"roughly uniformly distributed\" often means Gaussian with large variance, so in particular it applies (asymptotically) to log-normal distributed processes with diverging variance, such as stock prices, etc. - for a more in depth treatment of this, see [T.Tao on Benford's law](https://terrytao.wordpress.com/2009/07/03/benfords-law-zipfs-law-and-the-pareto-distribution/) and most importantly the two pictures in [this Wikipedia section](https://en.wikipedia.org/wiki/Benford%27s_law#Prevalence)\n",
    "* eigenvalues of large random matrices, most prominently **Wigner's semicircle law** (macroscopic) and the **local law** (microscopic; addresses the spacing between adjacent eigenvalues)\n",
    "* **SLEs**\n",
    "* **Poisson-Dirichlet** for the distribution of random partitions in a surprisingly large number of models including [prime factorisation and cycle decomposition of permutations](https://terrytao.wordpress.com/2013/09/21/the-poisson-dirichlet-process-and-large-prime-factors-of-a-random-number/), as well as many other \"random loop ensembles\" (see e.g. [my presentation here](http://peter.muehlbacher.me/files/talk_loopsoups.pdf)); many applications in investigating *phase transitions*\n",
    "\n",
    "## digression on applications\n",
    "Note that, even though they sound similar at first, the CLT and Benford's law are actually rather different in the sense that the CLT carries more information (a Gaussian is the first order approximation of large random sums, other contributions are of lower order, i.e. disappear after rescaling by $\\sqrt n$); no such rescaling is required for Benford's law, in fact this basically reduces to the trivial statement that variance of the sum of (sufficiently independent) random variables (=the log of their product) diverges. Now this may explain why it is not quite as popular as the CLT in pure mathematics (and why it holds very, very generally), but it certainly makes up for this with a surprisingly large number of very neat applications: mostly detecting whether some statistics (in the broader sense; like elections, etc.) are faked. Explaining inconsistency with Benford's law seems to be much harder than some other tests for, say, election fraud; see e.g. [turnout – voting-behaviour correlation analysis](https://arxiv.org/pdf/1706.09839.pdf) (one might be tempted to argue that people in areas with higher turnout simply received a disproportionate amount of ads from one side) and many others.\n",
    "\n",
    "## further notes\n",
    "This will certainly be expanded in the future; T.Tao also wrote on this: [his first draft](https://terrytao.wordpress.com/2010/09/07/a-first-draft-of-a-non-technical-article-on-universality/) and [his second draft](https://terrytao.wordpress.com/2010/09/14/a-second-draft-of-a-non-technical-article-on-universality/) (less technical, more sources linked at the end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [[[write it]]] field theory for bosons\n",
    "https://ocw.mit.edu/courses/physics/8-08-statistical-physics-ii-spring-2005/lecture-notes/fieldtheory_bos.pdf\n",
    "\n",
    "## [Wick's theorem (derivation)](https://math.stackexchange.com/questions/126227/reference-for-multidimensional-gaussian-integral)\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Trace of non-negative, s.a. integral operator $K$](https://math.stackexchange.com/questions/2336561/trace-of-non-negative-self-adjoint-integral-operator)\n",
    "* note that we cannot just consider $\\int K(x,x)dx$ because the diagonal is a set of measure zero, hence $K(x,x)$ is not well-defined\n",
    "* consider $A$ the positive square root of $K$, i.e. $A\\geq0$ and $A^2=K$ with square integrable kernel $a(x,y)$\n",
    "* note that\n",
    "  * $Tr(K)$ is equal to the trace class norm $\\|K\\|_1$\n",
    "  * the Hilbert-Schmidt norm $\\|A\\|_2$ is well-defined since the root of every trace class operator is H-S\n",
    "* then $$\\|K\\|_1=\\|A\\|_2^2 := \\int\\int |a(x,y)|^2dxdy$$\n",
    "* since $A$ is also self-adjoint we have $\\overline{a(x,y)}=a(y,x)$ and hence $$\\int\\int |a(x,y)|^2dxdy = \\int\\int a(x,y)a(y,x)dxdy$$\n",
    "* we could define $k(x,x):= \\int a(x,t)a(t,x)dt$ to make sense of the intuition from the finite-dimensional (matrix) case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# necessary and sufficient condition for *orthogonal* projections\n",
    "\n",
    "found in this [quantum information paper](https://arxiv.org/pdf/1308.6595.pdf):\n",
    " * $\\Pi^*\\Pi = \\Pi \\Leftrightarrow \\Pi$ is an orthogonal projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [The Feynman-Kac formula](http://math.arizona.edu/~faris/talks/FKac.pdf)\n",
    "\n",
    "* interpretation of potential as killing rate in the continuum setting\n",
    "* proof of Faynman-Kac formula using Trotter's product formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Log-correlated Gaussian fields](https://arxiv.org/pdf/1407.5605.pdf)\n",
    "* relation to GFF (for $W$ = white noise)\n",
    "  * $GFF = (-\\Delta)^{-1/2}W$, for more in DGFF see e.g. https://eventuallyalmosteverywhere.wordpress.com/2016/11/03/dgff1/\n",
    "  * $LGF = (-\\Delta)^{-d/4}W$ in $d$ dimensions\n",
    "  * the LGFs with $d\\in\\{2, 1\\}$ coincide with the 2D GFF and its restriction to a line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Analytic combinatorics](http://algo.inria.fr/flajolet/Publications/book.pdf)\n",
    "[[[to be written]]]\n",
    " * transfer matrix models $\\leftrightarrow$ p356\n",
    " * [symbolic method](https://en.wikipedia.org/wiki/Symbolic_method_%28combinatorics%29#The_Flajolet%E2%80%93Sedgewick_fundamental_theorem)\n",
    " * on random permutations: [Wikipedia](https://en.wikipedia.org/wiki/Random_permutation_statistics) and [Tao](https://terrytao.wordpress.com/2011/11/23/the-number-of-cycles-in-a-random-permutation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topology facts to keep in mind\n",
    "\n",
    "* $\\tau_1\\subseteq\\tau_2$:\n",
    "  * $\\tau_1$ coarser/weaker/smaller than $\\tau_2$ $\\Leftrightarrow$ $\\tau_2$ finer/stronger than $\\tau_1$\n",
    "  * $\\tau_1$ has more convergent nets than $\\tau_2$ $\\Leftrightarrow$ $\\tau_2$ has less convergent nets than $\\tau_1$\n",
    "  * $id_X:(X,\\tau_2)\\to(X,\\tau_1)$ is continuous $\\Leftrightarrow$ $id_X:(X,\\tau_1)\\to(X,\\tau_2)$ is open\n",
    "* $f : (X,\\tau_X) \\to (Y,\\tau_Y)$ continuous remains continuous if \n",
    "  * $\\tau_Y$ is replaced by some coarser topology $\\tau_Y'\\subseteq\\tau_Y$ $\\Leftrightarrow$ $\\tau_X$ is replaced by some finer topology $\\tau_X'\\supseteq\\tau_X$\n",
    "  * special case: functionals (i.e. $f:(X,\\tau)\\to(\\mathbb R,\\mathcal B_\\mathbb{R})$); the coarser $\\tau$ is, the fewer continuous functionals exist\n",
    "* given $V=(V,\\tau)$ a topological vector space, then - up to equivalence of norms - there is at most one norm $\\|\\cdot\\|$ one can put on $V$ such that $(V,\\|\\cdot\\|)$ is a Banach space whose topology is at least as strong as $\\tau$; [easy proof here](https://terrytao.wordpress.com/2016/04/22/a-quick-application-of-the-closed-graph-theorem/)\n",
    "  * in particular, there is at most one topology stronger than $\\tau$ that comes from a Banach space norm\n",
    "  * intuitively, the ``stronger topology\" requirement means less convergent nets/sequences, so e.g. $\\tau$ the topology generated by pointwise convergence, vs uniform convergence (which is induced by $\\|\\cdot\\|_\\infty$) - the requirement that $(V,\\|\\cdot\\|)$ is a Banach space is necessary for uniqueness (this keeps us from picking e.g. Sobolev space norms on $V$ the space of continuous functions)\n",
    "  * conversely, if one takes a space $V$ and its completion w.r.t. a weaker norm (e.g. $\\|\\cdot\\|_{L^2}$ weaker than $\\|\\cdot\\|_{H^1}$), one will get larger spaces\n",
    "  \n",
    "\n",
    "### topologies on ${\\displaystyle X=\\prod_{i\\in I}X_i}$\n",
    "\n",
    "|[product topology](https://en.wikipedia.org/wiki/Product_topology) | [box topology](https://en.wikipedia.org/wiki/Box_topology)\n",
    "---|:--- | :---\n",
    "base | $${\\displaystyle \\left\\{\\left.\\prod _{i\\in I}U_{i}\\ \\right|U_{i}{\\text{ open in }}X_{i}\\text{ and } U_i\\neq X_i\\text{ for finitely many }i\\right\\}}$$ = cylinder sets | ${\\displaystyle \\left\\{\\left.\\prod _{i\\in I}U_{i}\\ \\right|U_{i}{\\text{ open in }}X_{i}\\right\\}}$\n",
    "convergence, interpreting $X^S$ as $\\{f:S\\to X\\}$ | pointwise convergence (since we can only check for similarity on finitely many coordinates at once) | assuming $X$ is Hausdorff, $f_n\\to f\\in X^S \\Leftrightarrow$ $f_n$ converges pointwise and $\\exists$ a finite $S_0\\subseteq S$ and $N\\in\\mathbb N$ with $(f_n(s))_{n>N}$ is constant for all $s\\in S\\setminus S_0$ (since we can prescribe arbitrary speed of convergence for all coordinates at once)\n",
    "functionals|product topology is weaker than box topology $\\Rightarrow$ there are less continuous functionals; in particular continuous functionals from the product topology must not depend on infinitely many values (the primage of any open set in $\\mathbb R$ can be written as (union over) cylinder sets; hence at most finitely many coordinates can proper subsets of $X_i$)|\n",
    "theorems |[Tychonoff's theorem](https://en.wikipedia.org/wiki/Tychonoff%27s_theorem) for compactness, [Kolmogorov's extension theorem](https://en.wikipedia.org/wiki/Kolmogorov_extension_theorem) to extend finite dimensional marginals to the $\\sigma$-algebra induced by the product topology|box topology is very useful for providing counterexamples; many qualities such as compactness, connectedness, metrizability, etc., if possessed by the factor spaces, are not in general preserved in the product with this topology\n",
    "\n",
    "more on that: https://math.stackexchange.com/questions/448550/difference-between-the-behavior-of-a-sequence-and-a-function-in-product-and-box/448575#448575"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# http://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\n",
    "p.8-15 for a list of derivatives of matrix-valued functions, eigenvalues, traces, determinants, norms, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# small $L^2$ norm is \"equivalent\" to finite $H^k (=W_{k,2})$ norm for sufficiently high frequency support\n",
    "write $\\|f\\|^2_{L^2} = \\int |\\mathcal Ff(s)|^2ds$ by Parseval's theorem and then insert $(1+|s|^2)^{-k}(1+|s|^2)^k$ to get $$\\|f\\|^2_{L^2} = \\int (1+|s|^2)^{-k}(1+|s|^2)^k|\\mathcal Ff(s)|^2ds\\leq (1+M^2)^{-k}\\|f\\|_{H^k}^2$$ for $\\mathcal Ff(s)=0$ for $|s|>M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to invert (parts of) matrices / get row/column sums\n",
    "\n",
    "* the obvious choice: **[Cramer's formula](https://en.wikipedia.org/wiki/Cramer%27s_rule).** While the basic formulation gives an analytic expression for $x_i$ in $Ax=b$ with given $A$ and $b$ this can easily be extended to giving $A^{-1}_{ij}$ by solving for $x_i$ in $Ax=e_j$.\n",
    "  * while this nicely captures the aspect of a solution existing if and only if (\"the relevant part of\") $A$ is invertible since it uses determinants, it's not terribly useful in most applications since computing the determinant of $A$ with rows/columns replaced by other vectors usually doesn't make the problem any easier\n",
    "* the less obvious choice: **[block matrix inversion](https://en.wikipedia.org/wiki/Invertible_matrix#Blockwise_inversion)/Schur's complement formula.** (aka the \"random matrix theory way of inverting matrices\") While this approach works for the more general problem of getting an entire block of the inverse, chosing the block to be of dimension $1\\times 1$ also does the job of getting some specific entry (possibly after some row/column permutations) of the inverse. For example we get $$(A^{-1})_{11} = (A_{11}-b^\\top (A')^{-1}a)^{-1}$$ for $A=\\left[\n",
    "\\begin{array}{c|c}\n",
    "A_{11} & b^\\top \\\\\n",
    "\\hline\n",
    "a & A'\n",
    "\\end{array}\n",
    "\\right]$ for $A\\in\\mathbb R^{n\\times n}$, some vectors $a,b\\in\\mathbb R^{n-1}$ and $A'\\in\\mathbb R^{(n-1)\\times(n-1)}$. This is nice if we already know the inverse of some principal minor and/or $A$ is symmetric.\n",
    "* **Neumann series**: by conjugating with diagonal matrices assume wlog (in case the diagonal entries are non-zero) that $\\text{diag}(A)=\\mathbb 1$, then we can formally write $$A^{-1}=\\frac{1}{\\mathbb 1-(\\mathbb 1-A)} = \\sum_{n=0}^\\infty (\\mathbb 1-A)^n,$$ which can be shown to converge if $\\lim_n (\\mathbb 1-A)=0$, similarly to a geometric series. \n",
    "  * assuming furthermore that the matrix $A$ is positive definite, we can uniquely write it as $A=LL^*$ for $L=D+N$ given by the Cholesky decomposition and $D=\\text{diag}(L)$ and $N=L-D$ a nilpotent lower triangular matrix\n",
    "  * wlog (this time justified because the diagonal of the Cholesky decomposition is strictly positive) assume that $D=\\mathbb 1$ to get $A^{-1}=(LL^*)^{-1}=(L^{-1})^*L^{-1}$ with $$L^{-1} =\\frac{1}{\\mathbb 1+N} = \\mathbb 1-N+N^2-\\dots+(-1)^nN^n,$$ which terminates at (latest at) the matrix dimension $n$ since $N$ is nilpotent.\n",
    "  * this also gives us that the inverse of some diagonal $A$ with a \"small\" off-diagonal perturbation $N$ is still essentially diagonal (where $N$ is \"small\" e.g. if $N^m=0$ for some small $m$ and $\\sup_{xy}|N_{xy}|$ is close to zero)\n",
    "* to get **columns of inverse matrices** we can solve $Ax=e_i$ which might (at least numerically) be more efficient&stable than actually computing the entire inverse\n",
    "* to get the **sum of all entries of an inverse matrix** $\\sum_{ij}(A^{-1})_{ij}$ we might adapt the above solution to sum over all columns by solving $Ax=(1,\\dots,1)^\\top$ and taking the inner product $\\langle x,(1,\\dots,1)^\\top\\rangle$\n",
    "  * in the case of positive definite matrices $A$ [we notice that](http://www.cis.upenn.edu/~jean/schur-comp.pdf) $f(x)=\\frac{1}{2}x^\\top Ax+x^\\top b$ attains its minimum at $x^*=-A^{-1}b$ with $$f(x^*)=-\\frac{1}{2}b^\\top A^{-1}b.$$\n",
    "  * choosing $b=(1,\\dots,1)^\\top$ we basically get the $\\sum_{ij}(A^{-1})_{ij}$ up to some constant factor\n",
    "* **relating a resolvent to the resolvent of its inverse** if you know how to invert the matrix: https://en.wikipedia.org/wiki/Woodbury_matrix_identity\n",
    "  * in the scalar case this simplifies to $$\\frac{1}{a+z}=\\frac{1}{a}-\\frac{1}{a^2}\\frac{1}{\\frac{1}{a}+\\frac{1}{z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to multiply lists of matrices in numpy\n",
    " \n",
    "* ```AB = [a@B for a in A]``` is the probably the option that's easiest to understand, but using lists/for-loops/… is not the fast way of doing things with numpy, see https://jameshensman.wordpress.com/2010/06/14/multiple-matrix-multiplication-in-numpy/ which provides some workarounds for a fairly limited setting with a pretty ugly syntax\n",
    "* according to http://ajcr.net/Basic-guide-to-einsum/ and several stackexchange threads ``einsum`` seems to be the fastest way of doing things in most cases and comes with a surprisingly decent looking syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Computational and Statistical Tradeoffs via Convex Relaxation](https://arxiv.org/pdf/1211.1073.pdf)\n",
    "\n",
    "* problem: more data = better statistics, but also worse runtime\n",
    "* this paper relates runtime to convex geometry and\n",
    "  >demonstrate[s] the efficacy of this methodology in statistical estimation in providing concrete time-data tradeoffs in a class of denoising problems\n",
    "* >convex relaxation offers a principled approach to exploit the statistical gains from larger datasets to reduce the runtime of inference algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient descent is not (asymptotically) optimal for smooth, convex optimization\n",
    "\n",
    "* within the class of descent methods (roughly speaking: discrete time algorithms where the next step has to be in the linear span of previous points and their gradients) the classical gradient descent algorithm (Euler discretization of gradient descent) has a rate of convergence of $O(1/k)$, i.e. after $k$ steps it is guaranteed to be within $O(1/k)$ of the minimum\n",
    "* Russian mathematicians (Nesterov at al.), however, have proved that the theoretical optimal rate of convergence is actually $O(1/k^2)$ and Nesterov even published the so-called \"accelerated gradient descent\" algorithm achieving this lower bound, although nobody really knows why it works so well, other than it comes out right in the calculations\n",
    "* https://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/ for more a more in-depth exposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# different characterisations of subgausian random variables and their relation to subexponential ones\n",
    "\n",
    "* any of the following (equivalent) conditions can be taken as definition of $X$ being subgaussian\n",
    "  * there exists an $a>0$ such that for all $t\\in\\mathbb R$ $$\\mathbb E e^{tX}\\leq e^{at^2/2}$$\n",
    "  * there exists a $b>0$ such that for all $\\lambda>0$ $$\\mathbb P(|X|>\\lambda)\\leq 2e^{-b\\lambda^2}$$\n",
    "  * there exists a $c>0$ such that $$\\mathbb Ee^{cX^2}\\leq 2$$\n",
    "  * $X^2$ is subexponential\n",
    "  \n",
    "  if $X$ is furthermore assumed to be centered the above conditions are also equivalent to the following moment condition:\n",
    "  * there exists a $d>0$ such that for all $p\\geq 1$ we have $$(\\mathbb E|X|^p)^{1/p}=\\|X\\|_p \\leq d\\sqrt p$$\n",
    "  \n",
    "  to see that last two equivalences see https://www.math.uci.edu/~rvershyn/papers/non-asymptotic-rmt-plain.pdf (page 9ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# when not to use Cauchy Schwarz\n",
    "\n",
    "When having to bound probabilities like $\\mathbb P(\\|Hx\\|>\\lambda)$ for a $N\\times N$ random matrix $H$ with centered entries and a unit vector $x:\\|x\\|=1$ it can be tempting to start working with the row vectors $(H_{ij})_{j=1}^N$ by considering $$\\mathbb P\\left(\\left(\\sum_j H_{ij}x_i\\right)^2 > \\lambda \\right) = \\mathbb P\\left(\\langle H_{i,\\cdot},x\\rangle_{\\mathbb C^N}^2 > \\lambda \\right)\\leq \\mathbb P\\left(\\|H_{i,\\cdot}\\|^2\\|x\\|^2 > \\lambda \\right) = \\mathbb P\\left(\\|H_{i,\\cdot}\\|^2 > \\lambda \\right),$$\n",
    "where we used Cauchy Schwarz and the obvious fact that something that's larger is more likely to be larger than a fixed value for the inequality.\n",
    "\n",
    "While this simplifies nicely we actually lose a lot. This becomes increasingly obvious if we let $N\\gg 1$, since for large $N$ we might expect (for sufficiently \"decorrelated\" entries $H_{ij}$ - remembering that they are assumed to be centered) some kind of CLT to hold. Just take $x_j=1/\\sqrt N\\; \\forall j$ and assume $H_{ij}$ to be iid. Then $$\\langle H_{i,\\cdot},x\\rangle_{\\mathbb C^N} = \\frac{1}{\\sqrt N}\\sum_j H_{ij}$$\n",
    "**is Gaussian and in particular of order $O(1)$**. Cauchy Schwarz, however, is only sharp if the vectors are parallel, which is increasingly unlikely in higher dimensions (i.e. as $N\\gg 1$), so we expect it to lose out on a lot as $N\\gg 1$. Indeed, proceeding as above by using Cauchy Schwarz to bound $\\langle H_{i,\\cdot},x\\rangle_{\\mathbb C^N}^2$ would give $$\\langle H_{i,\\cdot},x\\rangle_{\\mathbb C^N} \\leq \\sqrt{\\sum_j H_{ij}^2}$$ which (by taking expectation) **should be expected to be of order $O(\\sqrt N)$** (assuming finite variance of $H_{ij}$).\n",
    "\n",
    "**TL;DR:** Cauchy Schwarz seems to behave badly in high dimensions, in particular if the original expression can be expected to have a lot of cancellations, which are the reason we need to rescale by $\\sqrt N$ and not by $N$ in the CLT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic results from extreme value theory\n",
    "* $\\sup_{n\\leq k} X_n ~ \\sigma\\sqrt{2\\ln k}$, $X_n$ being Gaussian with variance $\\sigma$\n",
    "* fluctuations are of inverse order $(\\sigma\\sqrt{2\\ln k})^{-1}$\n",
    "* **Q: is $\\mathbb E\\sup_{n\\leq k} X_n$ always asymptotically equivalent to the inverse function of the tails?** in (sub-)Gaussian case the tail $\\mathbb P(X>t)$ decays like $ce^{-Ct^2}$ and $\\sup_{n\\leq k} X_n ~ c\\sqrt{C\\ln k}$, similarly for a collection of iid exponentially distributed random variables (see [here](https://www.stat.berkeley.edu/~mlugo/stat134-f11/exponential-maximum.pdf))\n",
    "* \"A\": very reasonable: to see this write (for $X_i \\sim X$ iid and $X_n^*:=\\max_{1\\leq i\\leq n} X_i$) $$\\mathbb P\\left(X_n^* > t\\right) = \\mathbb P\\left(\\bigcup_{i=1}^n\\{X_i>t\\}\\right) = 1-(1-\\mathbb P(X>t))^n$$\n",
    "choosing $t=t_n=\\mathbb EX_n^*$ we expect the the above to evaluate to some non-trivial value for all $n$ (in fact uniformly, in the sense that as $n\\rightarrow\\infty$ we don't expect $\\mathbb P(X_n^*>\\mathbb EX_n^*)$ to go to zero or one).\n",
    "Denoting the tail function $\\mathbb P(X>t)$ by $F_X(t)$, this is (for example) the case if $(1-F_X(\\mathbb EX_n^*))^n\\rightarrow a\\in (0,1)$, which is the case if $F_X(\\mathbb EX_n^*) = O(1/n)$ as can be seen by the definition of $e^x$ as $\\lim_n (1+x/n)^n$ for $x \\approx -1$.\n",
    "This happens to be the case if $F_X$ is asymptotically (on an exponential scale) equal to the inverse of $\\mathbb EX_n^*$ (e.g. in the exponential case we have (ignoring all the constants): $\\mathbb EX_n^* = \\log n$ and $F_X(t)=e^{-t}$, yielding $F_X(\\mathbb EX_n^*) = -1/n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace transform\n",
    "* $\\mathcal L(\\mu)(s):=\\int e^{-st}d\\mu(t)=\\mathbb E[e^{-sX}]$, where the integral is usually taken over $[0,\\infty)$ or $\\mathbb R$ (which is referred to as the two-sided Laplace transform) and $X$ is a random variable with distribution given by $\\mu$\n",
    "* considering the **special case of $\\mu$ being a probability density**, i.e. setting $d\\mu(x) = f(x)dx$, we recover the non-measure theoretic definition of the Laplace transform, which we will denote by $\\mathcal L(f)$\n",
    "* note the **similarity to the Fourier transform** (with imaginary argument), so as one might expect, it can easily be shown that $$\\mathcal L(f'(t)) = s\\mathcal L(f)(t)$$ which holds for the two-sided Laplace transform, which we're interested in anyway unless we explicitly want to study non-negative random variables - for the usual Laplace transform a similar result can be shown\n",
    "* the following two results follow easily:\n",
    "    * **moments**: denote the $n$-th moment of a random variable $X$ with PDF $f$ by $m_n:=\\int x^nf(x)dx$, then $$m_n = (-1)^n(\\mathcal L(f))^{(n)}(0) = (-1)^n\\frac{d^n}{ds^n}\\mathbb E[e^{-sX}]_{s=0}$$\n",
    "      * this can easily be remembered (/formally derived) by writing $\\mathbb E[e^{-sX}]=\\mathbb E[1-sX+(sX)^2/2!-\\dots]$\n",
    "    * **cumulative distribution function**: to get the CDF $F_X$ of a continuous random variable $X$, given only its moment generating function (=Laplace transform) $\\mathbb E[e^{-sX}]$, we just need to calculate $$F_X(x)=\\mathcal L^{-1}\\left(\\frac{1}{s}\\mathbb E[e^{-sX}]\\right) = \\mathcal L^{-1}\\left(\\frac{1}{s}\\mathcal L(f)(s)\\right)$$\n",
    "        * in practive computing $\\mathcal L^{-1}$ can be done by using the Cauchy residue theorem, see [Wikipedia](https://en.wikipedia.org/wiki/Inverse_Laplace_transform)\n",
    "* **resolvents**: a formal calculation (which can be made precise by Hille-Yosida theorem) easily gives that for hermitian $A$ and $U(t):=e^{itA}$ (a family a unitary operators) we have $$\\int_0^\\infty e^{-zt}U(t)dt = (iA-zI)^{-1} =: \\text{Res}(z;iA)$$\n",
    "  * to see this, write $$\\int_0^\\infty e^{-zt}U(t)dt = \\int_0^\\infty e^{t(iA-z)}dt,$$ substitute $iA-z$ with $s$ to get $$\\int_0^\\infty e^{st}dt = e^{st}/s|_{t=0}^{t=\\infty}=-1/s,$$ iff $e^{t(iA-z)}\\rightarrow 0$, which is the case iff $e^{-tz}\\rightarrow 0$, iff $\\text{Re}(z)>0$, since $e^{itA}$ is unitary\n",
    "  * in the same spirit it can be seen that $$e^{tA} = \\mathcal L^{-1}\\left(\\frac{1}{\\lambda-A}\\right)$$ for $A$ bounded and $\\lambda>\\text{Re}(A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# easy bounds for factorials\n",
    "\n",
    "* lower bound: $n!\\geq \\left(\\frac{n}{e}\\right)^n$\n",
    "  * proof: considering Taylor series of $\\exp$ we get $e^x\\geq \\frac{x^n}{n!}$ - now take $x=n$ and rearrange\n",
    "* upper bound: $n!\\leq n^n$ (obvious)\n",
    "* Stirling: $$\\sqrt{2\\pi n}\\left(\\frac{n}{e}\\right)^n \\leq n!\\leq e\\sqrt n\\left(\\frac{n}{e}\\right)^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topological space $X$ is Hausdorff iff its diagonal is closed in the product topology\n",
    "https://math.stackexchange.com/questions/136922/x-is-hausdorff-if-and-only-if-the-diagonal-of-x-times-x-is-closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TO BE WRITTEN]\n",
    "\n",
    "proof of $\\mathbb E e^{\\lambda |f(X)-\\mathbb Ef(X)|}\\leq \\mathbb E e^{\\frac{\\lambda \\pi}{2}|\\langle\\nabla f(X),Y\\rangle|}$ - sampling another independent version of X, doing integral tricks to get bounds\n",
    "\n",
    "**[[[follow Gaussian processes lecture notes]]]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# showing almost sure equality of random variables of which we have control over $L^p$ norms of their differences [TO BE FINISHED]\n",
    "We start with the following simple observations:\n",
    "\n",
    "* For $\\|X-Y\\|_p=0$ we have $X=Y$ almost surely.\n",
    "* Now assume $X_n\\rightarrow X$ in $L^p$ and $X_n$ converges to some limit $Y$ almost surely. It then follows from the fact that $L^p$ convergence implies convergence in probability, which, in turn, implies almost sure convergence of a subsequence, that $X=Y$ almost surely.\n",
    "\n",
    "Assume you are given a sequence $X_n$ and want to show that $X_n\\rightarrow X$ almost surely\n",
    "\n",
    "**[[[proceed as in stochastic analysis notes]]]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X_n\\rightarrow X$ in $\\mathbb P$ implies $\\mathbb E[|X_n-X|^p] \\rightarrow 0$ for any $p\\geq 1$ if $X_n$ is bounded almost surely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://math.stackexchange.com/a/15296/33692\n",
    "\n",
    "nice trick using MCT/DCT (with counting measure) to derive sufficient conditions on switching limits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [people who win [machine learning] competitions are consistently machine learning experts, not domain experts](https://news.ycombinator.comhttps://news.ycombinator.com/item?id=12731301/item?id=12731301)\n",
    "\n",
    "* Notably: https://www.kaggle.com/c/MerckActivity\n",
    "  > Since our goal was to demonstrate the power of our models, we did no feature engineering and only minimal preprocessing. The only preprocessing we did was occasionally, for some models, to log-transform each individual input feature/covariate. Whenever possible, we prefer to learn features rather than engineer them. This preference probably gives us a disadvantage relative to other Kaggle competitors who have more practice doing effective feature engineering. In this case, however, it worked out well.\n",
    "* http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/\n",
    "  > Q: Do you have any prior experience or domain knowledge that helped you succeed in this competition? A: In fact, no. It was a very good opportunity to learn about image processing.\n",
    "* http://blog.kaggle.com/2016/09/15/draper-satellite-image-chronology-machine-learning-solution-vicens-gaitan/\n",
    "  > Do you have any prior experience or domain knowledge that helped you succeed in this competition? I didn't have any knowledge about this domain. The topic is quite new and I couldn't find any papers related to this problem, most probably because there are not public datasets.\n",
    "* http://blog.kaggle.com/2015/09/16/icdm-winners-interview-3rd-place-roberto-diaz/\n",
    "  > Do you have any prior experience or domain knowledge that helped you succeed in this competition? M: I have worked in companies that sold items that looked like tubes, but nothing really relevant for the competition. J: Well, I have a basic understanding of what a tube is. L: Not a clue. G: No.\n",
    "* http://blog.kaggle.com/2015/09/16/icdm-winners-interview-3rd-place-roberto-diaz/\n",
    "  > We had no domain knowledge, so we could only go on the information provided by the organizers (well honestly that and Wikipedia). It turned out to be enough though. Robert says it cannot happen again, so we’re currently in the process of hiring a marine biologist ;).\n",
    "* http://blog.kaggle.com/2015/09/22/caterpillar-winners-interview-1st-place-gilberto-josef-leustagos-mario/\n",
    "  > Through Kaggle and my current job as a research scientist I’ve learnt lots of interesting things about various application domains, but simultaneously I’ve regularly been surprised by how domain expertise often takes a backseat. If enough data is available, it seems that you actually need to know very little about a problem domain to build effective models, nowadays. Of course it still helps to exploit any prior knowledge about the data that you may have (I’ve done some work on taking advantage of rotational symmetry in convnets myself), but it’s not as crucial to getting decent results as it once was.\n",
    "* http://blog.kaggle.com/2016/08/29/from-kaggle-to-google-deepmind-an-interview-with-sander-dieleman/\n",
    "  > Oh yes. Every time a new competition comes out, the experts say: \"We've built a whole industry around this. We know the answers.\" And after a couple of weeks, they get blown out of the water.\n",
    "* http://www.slate.com/articles/health_and_science/new_scientist/2012/12/kaggle_president_jeremy_howard_amateurs_beat_specialists_in_data_prediction.html\n",
    "  > Competitions have been won without even looking at the data. Data scientists/machine learners are in the business of automating things -- so why should domain knowledge be any different?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering of noisy graphs + applications\n",
    "\n",
    "* https://arxiv.org/pdf/1702.00467.pdf an introduction\n",
    "* http://science.sciencemag.org/content/347/6224/1257601/tab-pdf \"Uncovering disease-disease relationships through the incomplete interactome\" - how to use methods from physics devised for studying noisy graphs and their phase transitions to classify diseases \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://www.macs.hw.ac.uk/~simonm/ae.pdf\n",
    "\n",
    "nice introduction to asymptotic series and how to apply them to Laplace integrals (p21 ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# on the σ-algebra induced by finite-dimensional distributions\n",
    "\n",
    "* let $(\\Omega,\\mathcal A,\\mathbb P)$ be a probability space\n",
    "* a stochastic process is a function $X:\\Omega\\times T\\rightarrow\\mathbb R$, s.t. $X(\\cdot,t)=:X(t)$ is a random variable for all $t\\in T$ (i.e. is $\\mathcal A-\\mathcal B_{\\mathbb R}$-measurable, where $\\mathcal B_{\\mathbb R}$ is the Borel σ-algebra on $\\mathbb R$)\n",
    "* using [Kolmogorov's extension theorem](https://en.wikipedia.org/wiki/Kolmogorov_extension_theorem) we see that to define a stochastic process $X$ on some possibly uncountable index set $T$ it suffices to specify its finite-dimensional distributions (which have to satisfy some consistency relations), i.e. viewing $X(\\cdot)$ as some random variable on $\\mathbb R^T$, it suffices to specify the probability of cylinder sets $\\prod_{t\\in T}A_t$, where only finitely many $A_t\\in\\mathcal B_{\\mathbb R} s.t. A_t\\neq\\mathbb R$\n",
    "* call the σ-algebra induced by those cylinder sets $\\mathcal C$, the \"cylindrical σ-algebra\"\n",
    "* it is often said (e.g., see the Wikipedia article on [Kolmogorov's extension theorem](https://en.wikipedia.org/wiki/Kolmogorov_extension_theorem)) that this algebra is not \"very rich\", but what does that mean?\n",
    "* see https://fabricebaudoin.wordpress.com/2012/03/24/lecture-2-measure-theory-in-function-spaces/, the first exercise which says that we cannot measure (the probability of) events like \"all functions from $T$ to $\\mathbb R$ of which the supremum is $\\leq 1$\"\n",
    "* instead we have to restrict ourselves to a smaller class of functions, e.g. continuous/cadlag ones. why we have to do that becomes apparent when trying to prove why events like above are not in $\\mathcal C$:\n",
    "  * take e.g. $T=[0,1]$ (but any other uncountable $T$ will do as well), then the above problem can be phrased as the question whether $\\prod_{x\\in T}(-\\infty,1]$ is in $\\mathcal C$\n",
    "  * remember that $\\mathcal C$, the σ-algebra generated by cylinder sets, is the smallest σ-algebra containing all those cylinder sets\n",
    "  * for a σ-algebra we only need complements and _countable_ unions (hence also only countable intersections) to be in the set, so we only get \"control over _countably_ many coordinates\", i.e. sets of the form $\\prod_{t\\in T'}(-\\infty,1]\\times\\prod_{t\\in T\\setminus T'}\\mathbb R$, where $T'\\subset T$ is countable\n",
    "* so to be able to measure \"global\" events like the set of all functions (in some given function space) having some property that depends on every single point we need to restrict ourselves to function spaces where we can control this behaviour by only controlling countably many points, e.g. continuous functions\n",
    "\n",
    "## stochastic process $X$ sufficiently well-behaved → can see it as random variable taking values in function spaces with their natural topology\n",
    "\n",
    "* let $X$ be s.t. it takes values in the space of $d$-uniformly continuous functions $C_u(T,d)=C_u$, where $d$ is some metric on $T$, where $T$ is totally bounded or separable\n",
    "* obviously $C_u\\subset \\mathbb R^T$, so we can think of $X:\\Omega\\times T\\rightarrow\\mathbb R$ as element of $\\mathbb R^{\\Omega\\times T}=(\\mathbb R^T)^\\Omega$, so a function $X':\\Omega\\rightarrow \\mathbb R^T, \\omega\\mapsto X(\\omega,\\cdot)$ - **if this map was measurable we could identify $X$ with the *random variable* $X'$ taking values in the space of functions from $T$ to $\\mathbb R$**\n",
    "* to make sense of \"$X'$ being measurable\", one has to specify the σ-algebras on $\\Omega$ and $R^T$ though\n",
    "* for $\\Omega$ we stick to the natural choice $\\mathcal A$\n",
    "* endow $C_u$ with the (suitable restriction of the) cylindrical σ-algebra $\\mathcal C$ (restriction because $C_u\\subset \\mathbb R^T$, in general one cannot simply endow a subspace with the bigger space's σ-algebra - by abuse of notation let us simply call this $\\mathcal C$ again though)\n",
    "* given some consistent family of finite-dimensional distributions we can use Kolmogorov's extension theorem like above to obtain a unique probability measure $\\mu:\\mathcal C\\rightarrow [0,1]$ which satisfies $\\mu(A)=\\mathbb P(X'^{-1}(A)) \\forall A\\in\\mathcal C$\n",
    "* note that the coordinate functions $X_t:\\Omega\\rightarrow\\mathbb R$ are random variables, i.e. measurable, by definition of a stochastic process, so a priori we could only state the previous statement for finite $T$, but thanks to Kolmogorov's extension we actually get that $X'$ is $(\\Omega,\\mathcal A)-(C_u,\\mathcal C)$-measurable, i.e. a random variable\n",
    "* (to see that note that for $\\mathbb P$ to be defined the preimage under $X'$ of every $A\\in\\mathcal C$ has to be in $\\mathcal A$, which is just the definition of $(\\Omega,\\mathcal A)-(C_u,\\mathcal C)$-measurable)\n",
    "\n",
    "now the σ-algebra $\\mathcal C$ might not have anything to do with \"natural\" events, i.e. the σ-algebra induced by open sets (w.r.t. to some natural norm, like $\\|\\cdot\\|_\\infty$), so we show that it actually does coincide with the \"natural\" Borel σ-algebra on the space of uniformly continuous functions:\n",
    "\n",
    "* note that the space of continuous functions on a compact set endowed with $\\|\\cdot\\|_\\infty$ is a separable Banach space (Dudley - real analysis & probability, 11.2)\n",
    "* because of *uniform* continuity we can write $C_u(T,d)=C(\\overline{(T,d)})$, where $\\overline{(T,d)}$ is the completion of $T$ w.r.t. $d$, so it is a separable Banach space and we can find $T_0\\subset T$ countable s.t. $\\|f\\|_\\infty := \\sup_{t\\in T} |f(t)|=\\sup_{t\\in T_0}|f(t)|$\n",
    "* hence the closed balls $\\{f\\in C_u: \\|f-f_0\\|_\\infty\\leq r\\} = \\bigcap_{t\\in T_0}\\{f\\in C_u:|f(t)-f_0(t)|\\leq r\\}\\in \\mathcal C$, since $\\{f\\in C_u:|f(t)-f_0(t)|\\leq r\\}\\in \\mathcal C$ and $T_0$ countable \n",
    "* morever, in a separable metric space every open set is a countable union of closed balls so all open sets are in $\\mathcal C$ and hence the Borel σ-algebra $\\mathcal B_{C_u(T,d),\\|\\cdot\\|_\\infty}$ (of $C_u(T,d)$ w.r.t. the topology generated by $\\|\\cdot\\|_\\infty$) is a subset of $\\mathcal C$\n",
    "* the converse inclusion ($\\mathcal C\\subset \\mathcal B_{C_u(T,d),\\|\\cdot\\|_\\infty}$) can easily be seen using arguments similar to the one with closed balls before\n",
    "* hence we have $\\mathcal C = \\mathcal B_{C_u(T,d),\\|\\cdot\\|_\\infty}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mathematics & brain activity\n",
    "\n",
    "## http://www.pnas.org/content/113/18/4909.abstract\n",
    "\n",
    "> mathematical judgments were related to an amplification of brain activity at sites that are activated by numbers and formulas in nonmathematicians, with a corresponding reduction in nearby face responses\n",
    "\n",
    "## http://journal.frontiersin.org/article/10.3389/fnhum.2014.00068/full\n",
    "> the experience of mathematical beauty correlates parametrically with activity in the same part of the emotional brain […] as the experience of beauty derived from other sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hölder's inequality to bound expressions of the form $\\prod_i \\mathbb E[|X|^{n_i}]$\n",
    "\n",
    " * let $0<n_i<m$ s.t. $\\sum_i n_i=m$ and define $p_i=\\frac{n_i}{m}, q_i=\\frac{p_i}{p_i-1}$\n",
    " * apply Hölder's inequality to $|X|^{n_i}$ and $1$ to get $\\mathbb E[|X|^{n_i}]\\leq \\mathbb E[|X|^m]^\\frac{n_i}{m}$\n",
    "   * note that taking $n_i$-th roots here yields monotonicity of $p$-norms.\n",
    " * multiplying this out gives $$\\prod_i \\mathbb E[|X|^{n_i}]\\leq \\mathbb E[|X|^m]$$\n",
    " * note that usually Hölder goes \"in the opposite direction\", i.e. bounding expectation of a product by a product of expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a blog about maths\n",
    "Given \"constraints\":\n",
    " 1. want a static site generator (in particular an automatically generated landing page with links (+summaries) to(/of) articles\n",
    " 1. hosting on Github (or something similar) with a reasonably easy way to update and upload new articles\n",
    " 1. want to use Markdown for text\n",
    " 1. want to use LaTeX syntax (not only LaTeX-like!) for formulas\n",
    " 1. it shouldn't look too bad (e.g. oversized formulae, etc.)\n",
    " \n",
    "<!--At first I tried Jekyll which excelled at 1,2,3, while falling short of fulfilling 4,5 properly (the reason being that Kramdown (but also all the other non-default Markdown parsers) was a pain when formulae including an underscore for indices appeared. Also the size of the formulae was pretty random (pretty sure one could work around that, but given the annoying first point I didn't bother).-->\n",
    "\n",
    "I found that IPython notebooks were perfect for parsing a combination of Markdown and LaTeX, parsing everything correctly and offering an export option.\n",
    "\n",
    "Potential options:\n",
    " * **Pelican.** (automatic) Works similarly like Jekyll (just a little bit more complicated in choosing&adjusting themes, editing metadata and previewing) and has a plugin for IPython support. [[Instructions how to set it up](https://www.dataquest.io/blog/how-to-setup-a-data-science-blog/)]\n",
    " * **Github.** (semi-automatic) If you don't mind their design just upload your blog posts as IPython files there and edit the README manually to get a \"landing page\".\n",
    " * **Manually.** (DIY) Just export IPython notebooks as HTML files and keep one HTML file updated as a landing page. May be a pain to work with, but at least you don't need hours of setting everything up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplacians in probability (TO BE WRITTEN)\n",
    "* [mixing times → graph mincut → Cheeger's constant] → find EFs of Laplacian\n",
    "* minimization of Dirichlet energy $\\mathcal E(f,g)=\\int \\nabla f\\nabla g$ → EFs of Laplacian (spectral gap), can be seen intuitively by partial integration or by an easy explicit calculation as in the mixing times notes\n",
    "* from those calculations we get a discrete $L^2$ and probabilistic analogue of Poincaré's inequality (interpret $\\|f\\|_2^2$ as $var(f)$), interesting since spectral gap gives the relation between local and global \"fluctuations\"\n",
    "* [relate spectral gap to mixing times → geometrical interpretation of the constant in Poincaré's inequality]\n",
    "* [inverse spectral problems like dimensionality reduction/manifold reconstruction]\n",
    "<!-- Belkin thesis p.17ff/20ff, https://en.wikipedia.org/wiki/Poincar%C3%A9_inequality -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [rotation+translation invariance → function of Laplacian](http://www.math.harvard.edu/~canzani/math253/Lecture1.pdf)\n",
    "* given an operator $S$ which is invariant under translations and rotations, i.e. $S(f\\circ \\tau) = (Sf)\\circ\\tau$, where $\\tau$ stands for translations/rotations\n",
    "* then $S$ is a function of the Laplacian, in particular in $\\mathbb R^d$ we have some $m$ and coefficients $\\{a_i\\}_i$ s.t. $S=\\sum_i^m a_iL^i$, where $L$ is the Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random stopping times to get limits of n-th roots of random expressions (and more)\n",
    "* http://peter.muehlbacher.me/math/2016/12/02/from-counting-trees-to-bounding-eigenvalues/\n",
    "* for more fun stuff to do with randomized stopping times (given some time homogeneous Markov chain) see http://arxiv.org/pdf/1407.6831.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# why is red green color blindness so abundant?\n",
    "* since males are much more likely to have it (6% of males vs 0.4% of females), it goes without saying that the following is not the only/“primary” reason for it, but at least it helps to see why red&green are the “problem colors” (as opposed to blue)\n",
    "* the mathematical setting:\n",
    "  * given some set $I$ of “natural images” (arrays with RGB triples as entries) there are certain patterns $X$ we absolutely have to recognize in order to survive (&recreate)\n",
    "  * letting the proportions $r,g,b$ (w.r.t. $r+g+b=1$) of the cells responding to the respective colors vary we get an optimization problem $\\min_{p,b}\\sum_{i\\in I, x\\in X}f_{p,b}(i,x)=:\\min_{p,b}F(p,b)$, where $f_{p,b}(i,x)$ is a penalty for not recognizing some feature $x$ in the image $i$ given $b$ cells for blue, $p$ for red and $1-p-b$ for green\n",
    "  * surprisingly, (according to an informal discussion with Gasper Tkacik iirc) when you run some simulations, the energy landscape F (as a function of $p,b$) has a clear global minimum in terms of $b$ (i.e. $dF/db$ is big), but the derivative w.r.t. $p$ almost vanishes - or in other words: in our world it seems to be very important to get the blue tones correct, but the rest is negligible (or at least it used to be before traffic lights became a thing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://www.cs.columbia.edu/cg/pdfs/1180993110-laplacian.pdf\n",
    "* paper discussing different variants of discrete Laplacians and their properties like symmetry, positivity, positive definiteness, locality and maximum principles\n",
    "* main result: there are graphs that do not admit all those properties simultaneously (argues with dual graphs)\n",
    "\n",
    "# eigenvalues of the discrete Laplacian (or generally translation invariant and symmetric operators)\n",
    "* 1D case: writing out the Laplacian in the standard basis as a matrix it is easy to see that it is a circulant matrix and thus diagonalised by the discrete Fourier transform\n",
    "* general case (take 1): see http://mathoverflow.net/a/2829/47059, which basically says that the eigenvectors(/functions) of the Fourier transform (exp) also happen to be eigenvectors(/functions) of the translation operator → diagonalize with DFT to get eigenvalues\n",
    "* (slightly less) general case (take 2): write the action of the (discrete) Laplacian on some vector as a convolution → we know that the Fourier transformation turns convolutions into multiplications → integral operator turns into a multiplication operator, i.e. is diagonalized\n",
    "* more conrete results on $\\mathbb Z$: from general theory about circulant matrices we know that the eigenvalues are just the Fourier transform of one row(/column since it’s symmetric) - since the Laplacian is assumed to be symmetric (i.e. even) the $\\sin$ terms vanish and we are left with something of the form $\\sum f \\cdot (1-\\cos)$ where the sum over ones comes from the main diagonal part of the Laplacian (which is s.t. every rowsum = 0, i.e. the sum over the other entries in the row)\n",
    "* on $\\mathbb Z^d$ in case of no “diagonal” contributions (i.e. $\\Delta f(0,0)$ is some function of $\\{(x,y) : x=0 \\;or\\; y=0\\}$ one can reduce it to the problem on $\\mathbb Z$ by writing $\\mathbb Z^d$ as the Cartesian (graph) product of lower dimensional copies, see http://math.stackexchange.com/questions/139735/eigenstructure-of-discrete-laplacian-on-uniform-grid\n",
    "* on general graphs (or $\\mathbb Z^d$ with “diagonal” contributions) it is better to interpret it as a convolution operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# existence of LDPs by subadditivity\n",
    "* assume $\\pi_n$ is some sequence of events we want to get a LDP for, e.g. $\\pi_n = P(S_n\\geq nu)$ for some fixed $u$ and $S_n$ being some sum of random variables\n",
    "* if we can show that $\\pi_{n+m}\\geq\\pi_n \\pi_m$ (subadditivity - check this e.g. for sum of independent r.v.), we can transform them to $p_n := -ln \\pi_n$ satisfying $p_{n+m}\\leq p_n p_m$ and use Fekete’s subadditive lemma ($a_n\\geq 0$ subadditive → $\\lim_{n\\rightarrow\\infty}a_n/n = \\inf_{n\\geq 1} a_n/n$ to show that the LDP exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# when are Chernoff bounds sharp?\n",
    "* Chernoff-type bounds are something of the form $P(X\\geq a)\\leq\\mathbb E[e^{tX}]/e^{ta}\\forall t>0$\n",
    "* this can easily be seen by writing $P(X\\geq a)\\leq P(e^{tX}\\geq e^{ta})$ and applying Markov’s inequality\n",
    "* a geometric interpretation:\n",
    "  * write the probability of some event $A$ as expectation of its indicator function $\\bf 1_A$\n",
    "  * bounding the probability of $X\\geq a$ with $e^{tX}\\geq e^{ta}$ should be thought of as some exponential function touching the outer left part of the indicator function - which is, a priori, obviously not a bound that one expects to be sharp\n",
    "* in the proof of Cramer’s theorem (LDP), however, we only look at random variables that are ``exponentially’’ concentrated around their mean (that’s why we need large deviations after all), so the intuition is that the Chernoff bound is sharp since the whole mass is concentrated just around the point where the exponential function touches the indicator function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geometric interpretation of the (conditional) expectation\n",
    "* introduce conditional expectation $\\mathbb E[X|\\mathcal G]$ as follows:\n",
    "  * let $X$ be an integrable random variable on some probability space $(\\Omega,\\mathcal A, P)$ \n",
    "  * let $\\mathcal G\\subset\\mathcal A$ be a sub-$\\sigma$-algebra\n",
    "  * conditional expectation of $X$ given $\\mathcal G$, $\\mathbb E[X|\\mathcal G]$ is *any* random variable $Y$ s.t.\n",
    "    * $Y$ is $\\mathcal G$-measurable\n",
    "    * $\\forall A\\in\\mathcal G: \\mathbb E[X\\bf 1_A]=\\mathbb E[Y\\bf 1_A]$\n",
    "* it can be seen that such a $Y$ exists and, up to $P$-null equivalence, is uniquely determined by those conditions\n",
    "* moreover we have all the \"typical\" properties of expectations like\n",
    "  * positivity: $X\\geq 0$ implies $\\mathbb E[X|\\mathcal G]\\geq 0$ $P$-a.s.\n",
    "  * linearity\n",
    "  * monotonicity\n",
    "  * Jensen’s inequality: $\\phi(\\mathbb E[X|\\mathcal G])\\leq\\mathbb E[\\phi(X)|\\mathcal G]$, $P$-a.s. for $\\phi$ convex and $X$ s.t. $\\mathbb E|X|<\\infty$, $\\mathbb E|\\phi(X)|<\\infty$\n",
    "  * importantly: $X$ being $\\mathcal G$-measurable, ($\\mathbb E|X|,\\mathbb E|Y|<\\infty$) implies $\\mathbb E[XY|\\mathcal G]=X\\mathbb E[Y|\\mathcal G]$, $P$-a.s., which, after identifying the deterministic ``random’’ variable $X(\\omega)=1 \\forall\\omega$ with $1\\in\\mathbb R$, gives the second axiom for an expectation in terms of free probability, i.e. $\\mathbb E[1|\\mathcal G]=1$ (the first one being linearity)\n",
    "* one interpretation of this conditional expectation is a geometric one which can be made precise under suitable technical assumptions ($X\\in L^2(\\Omega,\\mathcal A,P)$): $\\mathbb E[X\\mathcal G]$ is the orthogonal projection of $X$ from $L^2(\\Omega,\\mathcal A,P)$ on $L^2(\\Omega,\\mathcal G,P)$\n",
    "* to put this in words: start with some random variable (/function of degrees of freedom determined by $\\mathcal A$) $X$, then find the function $\\tilde X$ in the space of functions with fewer degrees of freedom (i.e. those given by $\\mathcal G$ which has to be a subset of $\\mathcal A$) that is ``closest’’ to the original one\n",
    "* to illustrate what is meant by ``degrees of freedom’’ it is instructive to look at the example $\\mathcal G=\\{\\emptyset,\\Omega\\}$; to be $\\mathcal G$ measurable now means that the pre-image of every open set in the domain of $X$ has to be an element of $\\mathcal G$, which, for this $\\mathcal G$, implies that there are only constant functions - the second property in the definition now gives that this constant value has to be the classical expectation\n",
    "* conversely, if we take a bigger $\\sigma$-algebra, we give ourselves more degrees of freedom for $\\tilde X$ and can, in a sense stay closer to the original $X$, which, from a probabilistic point of view, just means that more randomness is retained - so if you think of some filtration, encoding e.g. the ``knowledge’’ after flipping a coin for the $n$-th time, and the random variables $S_n=X_1+\\dots+X_n$ being the sum of all outcomes up to the $n$-th flip, conditioning on the first element of the filtration (i.e. the $\\sigma$-algebra $\\mathcal G_1=\\sigma(\\{(0,\\{0,1\\},\\{0,1\\},\\dots),(1,\\{0,1\\},\\{0,1\\},\\dots)\\})$) just means that $X_1$ is measurable, so $\\mathbb E[X_1+X_2+\\dots+X_n|\\mathcal G_1]=\\mathbb E[X_1|\\mathcal G_1]+\\mathbb E[X_2+\\dots+X_n|\\mathcal G_1]$ by linearity and $E[X_1|\\mathcal G_1]$ simplifies to $X_1$, whereas $\\mathbb E[X_2+\\dots+X_n|\\mathcal G_1]$ simply becomes the usual expectation $1/2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp of involutions\n",
    "* for any involution $a:a^2=I$, where $I$ is the identity, one has $exp(ca)=cosh(c)+a sinh(c)$ for scalar $c$, which can be seen by looking at the power series\n",
    "* can also be used for sin/cos with exp=i*sin+cos\n",
    "* can also be written as $cosh(c)(1+a tanh(c))$ which can be seen as “low temperature expansion” (think about $c$ as some temperature close to zero or more mathematically as some small $\\varepsilon>0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to get compact sets in non Euclidean settings\n",
    "* (one) idea: given some compact inclusion $i$ of space $X$ into $Y$, find some bounded set $B\\subset X$ and consider $i(B)$\n",
    "* this is useful for locally convex spaces of all sorts and works well with machinery like the Rellich–Kondrachov theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\{x : |x|_i<\\varepsilon, i\\in I\\}$ does not have to be an open set\n",
    "* take product topology (cylinder sets $\\{x : |x|_i<\\varepsilon, i\\in J\\}$ for $J$ being a finite subset of $I$ are a neighbourhood basis of $0$)\n",
    "* if $I$ is not finite, no element in the neighbourhood basis is contained in the set in the heading (if the norms $|\\cdot|_i$ do not trivially depend on each other) → it is not an open set\n",
    "* examples: locally convex spaces in PDEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# green, white, black and Oolong tea\n",
    "http://cooking.stackexchange.com/questions/26002/what-is-the-difference-between-green-white-and-black-tea for quick introduction (also Oolong tea)\n",
    "\n",
    "* fermentation: green&white tea leaves are quickly heated (typically steamed in Japan, while pan-fired in China) after harvesting to reduce oxidation\n",
    "* white tea\n",
    "  * minimally processed to leave downy hair intact\n",
    "  * not oxidized → does not develop as much flavor, color, or caffeine\n",
    "* green tea\n",
    "  * history\n",
    "    * starting from 17th century (Europeans started importing tea) until mid 19th century green tea was about as popular as black tea (in Europe)\n",
    "    * originally (2000 bc) used as medicine in China\n",
    "    * in the 8th century 陸羽 (Lù Yǔ) wrote 茶經 (chájīng), the first known monograph on tea where he described tea being steamed and formed into tea bricks for storage and trade - to prepare tea one had to pulverize it before brewing → 抹茶 (matcha)\n",
    "    * only later it was established\n",
    "  * rolling wilted leaves breaking cell walls to speed release of aromatic substances\n",
    "* types of tea\n",
    "  * China\n",
    "    * 珠茶 (gunpowder): rolled into little round pellets (green or Oolong tea); shiny pellets indicate freshness, little pellets are considered a mark of higher quality tea\n",
    "    * 龙井茶 (Lóng Jǐng tea): renowned for its high quality\n",
    "    * 黄山毛峰 ((Huángshān) Máo Fēng): mild-flavored, very popular tea\n",
    "    * 茉莉花茶 (Jasmine tea): subtly sweet and highly fragrant, stored with blossoms to acquire their scent\n",
    "    * 白牡丹 (Bái mǔdān): one of the most well-known white teas\n",
    "  * Japan\n",
    "    * 煎茶 (Sencha): most popular tea in Japan (80°C, 1 min, 1.5 tablespoons (7-8 grams) per litre)\n",
    "    * 茎茶 (Kukicha): blend made of stems, stalks, and twigs (80°C, 40sec to 1min, 4 teaspoons per litre)\n",
    "    * 抹茶 (Matcha): mentioned above\n",
    "    * 玉露 (Gyokuro): grown under the shade rather than the full sun since the more sun, the more Catechin (bitter)\n",
    "* aromatized tea: additional flavors, but basically loses its ability of being brewed more than once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://arxiv.org/pdf/1411.1792v1.pdf\n",
    "first few layers of CNNs with similar purposes tend to be very similar (Gabor filters) → can copy them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://arxiv.org/pdf/1301.3583v4.pdf\n",
    "increasing the size of neural networks only yields diminishing returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://arxiv.org/pdf/1312.5851v5.pdf\n",
    "how to implement the convolutions of CNNs with the FFT (feasible for large number of feature maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://math.stackexchange.com/questions/766479/what-is-spectrum-for-laplacian-in-mathbbrn, http://math.stackexchange.com/questions/790401/spectrum-of-laplace-operator\n",
    "two great articles covering why the spectrum of the Laplace operator (on $\\mathbb R^N$) is $(-\\infty,0]$ which, by looking more closely, also explains why “unbounded domain” implies “uncountable basis of eigenfunctions”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://cs231n.github.io/convolutional-networks/\n",
    "great introduction to convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://stackoverflow.com/questions/7536465/create-a-2d-array-with-a-nested-loop\n",
    "why you want to use `[[None for j in xrange(3)] for i in xrange(3)]` instead of `[[None]*3]*3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://arxiv.org/pdf/1405.4537v1.pdf (TO BE WRITTEN)\n",
    "* def: stream is a map $\\gamma$ from a totally ordered set $I$ to some state space\n",
    "* there is a canonical way to convert discrete streams to continuous paths (path: $I$ interval and some regularity conditions like right continuity)\n",
    "* from now on “wlog”: $\\gamma:[J_-,J_+]\\rightarrow E$ will continuously map an interval $J$ to some Banach space $E$\n",
    "* def: bounded $p$-variation ($p\\geq 1$) iff $sup_{\\dots<u_i < u_{i+1}<\\dots\\in J}\\sum_i\\|\\gamma_{u_{i+1}}-\\gamma_{u_i}\\|^q <\\infty$ for $q=1$ and $q=p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFT\n",
    "* DFT in $N$ dimensions can be written as $F_N=1/\\sqrt N (\\omega_N^{kl})_{k,l=0}^{N-1}$, where $\\omega_N=e^{2\\pi i/N}$ and this matrix is a Vandermonde matrix over the roots of unity\n",
    "* multidimensional DFT consists of iterated sums which commute → can write it as $F_{N_2}(F_{N_1}(X_{i,j})_{j=0}^{N_1-1})_{j=0}^{N_2-1}$\n",
    "* Vandermonde matrix = evaluation of a polynomial at points generating the Vandermonde matrix → upsampling = ($e^{2πinm/2N})_{n,m=0}^{n=N-1,m=2N-1}$ (Vandermonde matrix with twice as many rows as a square one → evaluates the polynomial described by the given vector at twice as many points of the unit circle (→ to convert a signal of frequency a to one with frequency b you first need to upsample it to LCM(a,b))\n",
    "* real signal x → DFT(x) has entries that are complex conjugated to each other (usually mirrored around half the length of the signal) since they are just the $L^2(\\mathbb C)$ inner product $(\\langle x,e_k\\rangle)_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linearisation of a differential operator\n",
    "http://math.stackexchange.com/questions/1677181/how-do-you-linearize-a-differential-operator-to-get-its-symbol for an example\n",
    "\n",
    "* used (i.a.) to define the symbol of a differential operator $D$\n",
    "* how to do it:\n",
    "  * usually linearize around a solution $\\tilde u, D\\tilde u=0$\n",
    "  * idea is to look at how $D$ behaves on functions $u$ that are close to the point one is linearising around (i.e. $\\tilde u$), so one does the substitution $u=\\tilde u+\\varepsilon v$\n",
    "  * rearranging terms like a power series in $\\varepsilon$ and using that $D\\tilde u=0$ one gets equations with $\\tilde u$ and $v$ which define an operator $A_{\\tilde u}$ acting on $v$, which satisfies $A_{\\tilde u}v = 0$ and is linear!\n",
    "* note that the linearisation depends on the solution just as the linearisation of a scalar valued function depends on the point $x_0$ one wants to linearize around and that the origin shifts accordingly, i.e. the input $v$ is much like $x-x_0$ and not $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://www-etud.iro.umontreal.ca/~sordonia/pdf/sigir2013_sordoni.pdf (TO BE WRITTEN)\n",
    "* problem: model dependencies of words\n",
    "  * either: need additional features (“computer”, “architecture” and “computer architecture” are completely different entries)\n",
    "  * or: model it as joint probabilities (less improvements than expected, huge computational effort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://math.ucr.edu/home/baez/rosetta.pdf\n",
    "great paper for everybody who is interested in (yet not familiar with) category theory - summary may follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://arxiv.org/pdf/cond-mat/0611023v1.pdf\n",
    "* distribution of eigenvalues of the Hessian of a critical point is a shifted semicircle\n",
    "* e.g. global minimum → left of SCL is at 0, the bigger its energy, the more it is shifted to the left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Braess’ paradox](http://ist.ac.at/fileadmin/user_upload/pdfs/Talks/2016/02/Talk_Timme.pdf)\n",
    "* the paradox: given some (directed) graph with a flow (e.g. traffic network) removing edges could improve the “overall situation” (e.g. on average you do not drive as long as before if a street is closed)\n",
    "* note that this also implies the converse: adding a street doesn’t necessarily make the traffic situation better overall\n",
    "* one take on this paradox (from a dynamical systems point of view): knowing the capacity of all the edges and the nodes one gets a system of constraints for every closed loop in order to satisfy some stability condition, but if one introduces another edge that divides a circle in two we get two systems of constraints that may not be compatible → no more stable solution (= traffic jam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://www.huffingtonpost.com/2015/05/13/andrew-ng_n_7267682.html\n",
    "* “The idea is that innovation is not these random unpredictable acts of genius, but that instead one can be very systematic in creating things that have never been created before. […] learn a lot, read a lot, talk to experts.”\n",
    "* on early influences: moved around, visited many different colleges and interned at different labs → many different points of view\n",
    "* don’t “follow your passion” (which usually gets amended to “follow your passion of all the things that happen to be a major at the university you’re attending”); often “you first become good at something, and then you become passionate about it. And I think most people can become good at almost anything. So when I think about what to do with my own life, what I want to work on, I look at *two criteria*. The first is whether it’s an *opportunity to learn*. Does the work on this project allow me to learn new and interesting and useful things? The second is the *potential impact*. **The world has an infinite supply of interesting problems. The world also has an infinite supply of important problems. I would love for people to focus on the latter.**”\n",
    "* “one pattern of mistakes I’ve made in the past, hopefully much less now, is doing projects where you do step one, you do step two, you do step three, and then you realize that step four has been impossible all along”\n",
    "* “if you seriously study half a dozen papers a week and you do that for two years, after those two years you will have learned a lot. […] But that sort of investment, if you spend a whole Saturday studying rather than watching TV, there’s no one there to pat you on the back or tell you you did a good job. Chances are what you learned studying all Saturday won’t make you that much better at your job the following Monday. There are very few, almost no short-term rewards for these things. But it’s a fantastic long-term investment. […] People that count on willpower to do these things, it almost never works because willpower peters out. Instead I think people that are into creating habits — you know, studying every week, working hard every week — those are the most important. Those are the people most likely to succeed.”\n",
    "* “I don’t work on preventing AI from turning evil for the same reason that I don’t work on combating overpopulation on the planet Mars”, more urgent: “[…] when the U.S. transformed from an agricultural to a manufacturing and services economy, we had people move from one routine task, such as farming, to a different routine task, such as manufacturing or working call service centers. A large fraction of the population has made that transition, so they’ve been okay, they’ve found other jobs. But many of their jobs are still routine and repetitive. The challenge that faces us is to find a way to scalably teach people to do non-routine non-repetitive work. Our education system, historically, has not been good at doing that at scale. The top universities are good at doing that for a relatively modest fraction of the population. But a lot of our population ends up doing work that is important but also routine and repetitive. That’s a challenge that faces our educational system.”\n",
    "* “One thing about speech recognition: most people don’t understand the difference between 95 and 99 percent accurate. Ninety-five percent means you get one-in-20 words wrong. That’s just annoying, it’s painful to go back and correct it on your cell phone. Ninety-nine percent is game changing. If there’s 99 percent, it becomes reliable.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://www.paulgraham.com/hs.html\n",
    "great text, you should read it!\n",
    "\n",
    "* if you don’t have a clear goal (which is usually the case) work forward from promising situations instead\n",
    "* if you have to choose, take those options that will give you the most promising range of options afterward\n",
    "* work on hard problems (“Writing novels is hard. Reading novels isn’t. Hard means worry: if you’re not worrying that something you’re making will come out badly, or that you won’t be able to understand something you’re studying, then it isn’t hard enough.”) → also get to know interesting people and get big ideas\n",
    "* “Put in time how and on what? Just pick a project that seems interesting: to master some chunk of material, or to make something, or to answer some question. Choose a project that will take less than a month, and make it something you have the means to finish. Do something hard enough to stretch you, but only just, especially at first. If you're deciding between two projects, choose whichever seems most fun. If one blows up in your face, start another. Repeat till, like an internal combustion engine, the process becomes self-sustaining, and each project generates the next one.”\n",
    "* “Don’t disregard unseemly motivations. One of the most powerful is the desire to be better than other people at something.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://terrytao.wordpress.com/2008/08/07/on-time-management/\n",
    "* “Another thing is that my ability to do any serious mathematics fluctuates greatly from day to day; sometimes I can think hard on a problem for an hour, other times I feel ready to type up the full details of a sketch that I or my coauthors already wrote, and other times I only feel qualified to respond to email and do errands, or just to take a walk or even a nap. I find it very helpful to organise my time to match this fluctuation: for instance, if I have a free afternoon, and feel inspired to do so, I might close my office door, shut off the internet, and begin typing on a languishing paper; or if not, I go and work on a week’s worth of email, referee a paper, write a blog article, or whatever else seems suited to my current levels of energy and enthusiasm. It is fortunate in mathematics that a large fraction of one’s work (with the notable exception of teaching, which one then has to build one’s schedule around) can be flexibly moved from one time slot to another in this manner. [A corollary to this is that one should deal with tasks before they become so urgent that they have to be done immediately, thus disrupting one’s time flexibility.]”\n",
    "* “A half-hearted system is probably worse than no system at all. A corollary to this is not to try to make an overly ambitious system ab nihilo that one is unlikely to follow faithfully; it is probably better to let such systems evolve over time.“\n",
    "* “Sometimes one should abandon one’s own rules and allow for serendipity. There have been many times, for instance, when I had planned to work on something during my lunch hour (grabbing something quick to eat), when I was interrupted by a colleague or visitor to go out to eat. It has often happened that I got a lot more out of that lunch (mathematically or otherwise) than I would have back at the office, though not in the way I would have anticipated.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://sivers.org/book/FluentForever\n",
    "first 625: http://fluent-forever.com/wp-content/uploads/2014/05/625-List-Thematic.pdf\n",
    "\n",
    "* start off with “minimal pair testing” (e.g. having to differentiate sounds like niece and knees)\n",
    "* use Google images, not translations\n",
    "* to memorize genders(/tones/…) imagine the masculine terms exploding, feminine catching fire, neuter shattering like glass (similar for tone colors)\n",
    "* good at remembering when images are violent/sexual/funny\n",
    "* cloze card types for functional words like “of”, “what”, …\n",
    "* for 10 ways to form plural, pick 10 nouns and use the person-action-object system (Tiger essen Fleisch)\n",
    "* also use this to learn verb/noun/adjective/adverb patterns\n",
    "* submit sentences to lang8 and put corrections in flash cards\n",
    "* TV series are easier than films (read ahead on Wikipedia, no subs)\n",
    "* card types should also ask for mnemonic (e.g. “What’s a phrase that includes [word]?”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://arxiv.org/pdf/1511.06444.pdf\n",
    "universality in halting time for spin glasses and deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://arxiv.org/pdf/1301.3537.pdf\n",
    "super short abstract:\n",
    "\n",
    "* pooling operators are for local invariance\n",
    "* 1 layer of convolutions is for learning 1 one-parameter-group invariance (or rather: convolutions leave already learned group invariance untouched and the training learns the invariance)\n",
    "* deep networks are basically doing group factorisation that’s stable w.r.t. perturbations of the group\n",
    "\n",
    "my notes:\n",
    "\n",
    "* the problem: find signal representation $\\Phi$ (e.g. classifier) that is\n",
    "\t* invariant under some transformation group $G$ (e.g. rotation), i.e. $\\Phi(x)=\\Phi(gx)$ for all $g\\in G$ and\n",
    "\t* is “stable” to perturbations (i.e. transformations $h$ that are “close” to the transformation group $G$ which can be thought of as a low dimensional manifold), i.e. $||\\Phi(\\varphi(h,x))-\\Phi(x)||\\leq C||x||d(h,G)$\n",
    "* approach: take convolutional networks and think of them in terms of signal processing, i.e.\n",
    "  * inputs $x\\in X$ = signals $x\\in L^2(\\Omega)$\n",
    "  * convolution kernels = filter bank $\\{\\psi_\\lambda\\}_\\lambda, \\lambda\\in\\Lambda_1$, e.g. for a filter bank corresponding to an expansion in a funtions Fourier series, convolution with $\\psi_\\lambda$ yields $\\langle x,e^{2\\pi i\\lambda/N}\\rangle$\n",
    "  * the first layer maps $x(\\cdot)\\mapsto \\{x*\\psi_\\lambda(\\cdot)\\}_\\lambda =: z^{(1)}(\\cdot,\\lambda)$ (which is the set of **convolutions** with kernels $\\psi_\\lambda$, applies some operator $M$ (the **activation function** which acts piece-wise) and then applies some **pooling operator** $P$, which, in terms of signal processing, can be thought of as low-pass filter (low-pass because chosing max-pooling can be thought of as eliminating high frequency oscillations), followed by downsampling (see Wikipedia for a quick explanation)\n",
    "* the special case of one-parameter transformation groups $G=\\{U_t\\}_{t\\in\\mathbb R}, U_t\\in L^2(\\Omega), \\lim U_t z = U_{t_0}z, U_{t+s}=U_tU_s$ (e.g. translations, frequency transpositions, dilations):\n",
    "  * we want to find a canonical way of describing the action of a group element of a one-parameter family; this is given by Stone’s theorem which (under reasonable assumptions) states that there is some s.a. $A$ such that $U_t=\\exp(itA)$ (spectral theorem!)\n",
    "  * since $A$ is self adjoint there is a change of basis given by $O$ that diagonalizes $A$, i.e. (in the finite dimensional case) $OAO^{-1} = \\text{diag}(\\lambda_1,\\dots,\\lambda_n)$\n",
    "  * hence we can write the group action $U_tz = O^{-1}(\\exp(it\\text{diag}(\\lambda_1,\\dots,\\lambda_n))Oz)$, implying that the group action is a linear phase change in the basis that diagonalizes $A$\n",
    "  * choosing $M$ to take the complex modulus yields a representation that is invariant under the action of $\\{U_t\\}_t$\n",
    "* “defining” perturbations of group elements:\n",
    "  * using the change of basis given by $O$ we can write the group action as some linear phase change; if we also apply the inverse Fourier transform this becomes a translation operator $T_s:z(\\cdot)\\mapsto z(\\cdot -s)$, which lets us measure deformations ($\\tilde T_s: z(\\cdot)\\mapsto z(\\cdot -\\tau(s))$ such that $d(\\tilde T_s,T_s)\\ll 1$) in a convenient way by analyzing the regularity of $\\tau$\n",
    "<!--  * thus the key to obtaining group invariant representations is not to find die eigenvectors of $A$ (yielding the change of basis given by $O$), but rather measurements that are “close” to diagonalising $A$ and are localised where deformations occur(?) - this can be done with convolutions with compactly supported filters-->\n",
    "I do not quite understand how this motivates the use of filters(/kernels) acting locally for group factorisation, but as soon as I do this will be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of the Fourier transform\n",
    "* diagonalises the derivative (i.e. turns it into a multiplication operator if expressed in its basis), which in turn let’s us define pseudo-differential operators via non-polynomial symbols\n",
    "* Fourier transform is like a projection onto eigenspaces of Laplace operator → in a compact domain (with suitable regularity) those are discrete → get a sum = Fourier _series_\n",
    "\t* using the above idea one can generalize Fourier transforms not only to higher dimensional setting and manifolds, but also to graph settings (see graph Laplacian)\n",
    "* translation = phase change (think of a shift in 1D and what this does to the coefficients of a Fourier _series_)\n",
    "  * in a space-time metric (having signature 1,1,1,-1) we can “place” a particle at some point in space-time by multiplying the creation operator with some exponential in the right basis (i.e. the one of the Fourier transform) where the spatial part has the inverse sign of the time part because of this metric\n",
    "* smoothness of the original function corresponds to decay in the Fourier domain (and vice versa), which can be made precise by the following statement about a random variable’s characteristic function (which is pretty much it’s Fourier transform)\n",
    "  * $\\mu({x: |x|\\geq 2/u})≤\\frac{1}{u} \\int_{-u}^u (1-\\hat \\mu(t))dt$\n",
    "  * in particular gives the following implication (in the appropriate setting): “characteristic function continuous in 0” → “measure is tight” (where tightness is useful since it enables application of Helly’s selection theorem, which is more or less the Banach-Alaoglu theorem for probability measures (latter would give weak-* convergence for signed, unnormed measures)\n",
    "* https://terrytao.wordpress.com/2009/04/06/the-fourier-transform/ for a more general take on the subject"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
